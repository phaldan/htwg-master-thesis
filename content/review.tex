\chapter{Review}
\label{ch:REVIEW}

Nachdem im Kapitel~\ref{ch:INITIAL} zuvor die Architektur von \textit{Zeta} im Ausganszustand beschrieben worden ist, werden in diesem Kapitel die einzelnen Teile der Architektur kritisch betrachtet. Dabei werden die vorhandenen Lösungen auf die Nutzung von Entwurfsmustern oder auch Design Prinzipien bewertet. Aber auch die Nutzung weiterer Entwurfsmuster wird evaluiert. Die Architektur wird zusätzlich auf das vorhanden sein von Anti-Pattern untersucht. Für etwaige kritische Teile werden jeweils unterschiedliche Lösungsvarianten erörtert.

Dieses Kapitel beginnt mit der \textit{Docker Compose} Umgebung und der Konfiguration der einzelnen Dienste. Drauf folgt eine Betrachtung der verschiedenen Unterprojekte innerhalb des Scala Projekts. Abgeschlossen wird das Ganze mit der Datenbank von Couchbase.

\section{Docker Compose Umgebung}

In diesem Abschnitt wird genauer auf die Konfiguration der Laufzeitumgebung für \textit{Zeta} per \textit{Docker Compose} eingegangen \cite{zeta_docker_compose}. Entsprechend werden die Konfiguration der jeweiligen Dienste innerhalb von \textit{Docker} Compose analysiert, aber auch die eigens für \textit{Zeta} definierten \textit{Docker Images}. Grundlage dieser Analyse sind die offiziellen Guidelines für \textit{Docker Compose} im Produktivbetrieb, die Best Practices zur Software Entwicklung mit \textit{Docker}, die Best Practices zur Erstellung eines \textit{Dockerfiles} und die Security by Design Prinzipien von \ac{owasp} \cite{docker_best_practices,docker_compose_production,dockerfile_best_practices,owasp_security_by_design}. Die Security by Design Prinzipien von \ac{owasp} kommen bei der Betrachtung der \textit{Docker Compose} Umgebung zum Einsatz, da diese Sammlung an Prinzipien sich zum Ziel gesetzt hat, sichere Architektur bei Erstellung von Webapplikation zu erreichen.

\subsection{Maßgeschneidertes Proxy Image}

Beginnend mit dem \textit{Proxy} als ersten Dienst in der \textit{Docker Compose} Konfiguration. Beim \textit{Proxy} Dienst wird eigens ein \textit{Docker Image} über ein \textit{Dockerfile} definiert. Entsprechend der Best Practices von \textit{Docker} sollte eine genaue Version beim genutzten Image von \textit{Nginx} angegeben werden, anstelle sich auf neusten Stand über das latest als Standard Tag zu verlassen. Somit wird eine Inkompatibilität durch Breaking Changes bei einer neuen Hauptversion verhindert und alle Entwickler nutzen dieselbe Version von \textit{Nginx} um auf allen Systemen die gleiche Umgebung zu haben.

Einer der Best Practises besagt die Gesamtgröße von \textit{Docker Images} und die Anzahl der Layer innerhalb eines Image sollten möglichst klein gehalten werden. Zu diesem Zweck sollten über die Definitionen in einem \textit{Dockerfile} nur die nötigsten Abhängigkeiten enthalten sein und auf unnötige Pakete sollte verzichtet werden. Zusätzlich ist auch die Größe eines \textit{Docker Image} immer ein relevanter Punkt. Der \textit{Proxy} Dienst nutzt das Standard \textit{Nginx} Image. Jedoch ist \textit{Nginx} auch in einer minimallen Variante mit \textit{Alpine} als Basis Image vorhanden und hat im Vergleich zum Standard Image nur ein Bruchteil der Größe. Die \textit{Alpine} Variante ist nicht mit sämtlich möglichen Szenarien kompatibel. Dennoch sollte die \textit{Alpine} Variante aufgrund kürzerer Downloadzeiten, Schonung der Bandbreite, schnelleres Starten von \textit{Docker Containern} und allgemein weniger Ressourcenverbrauch des Computers bevorzugt werden.

Weiterhin wird im \textit{Dockerfile} nur eine einzige Datei in das Image kopiert. Bei dieser Datei handelt es sich um die Konfigurationsdatei der \textit{Virtual Hosts} innerhalb von \textit{Nginx}. Die Konfigurationsdatei von \textit{Nginx} kann auf einem lokalen Entwicklungssystem auch direkt per \textit{Bind Mount} eingebunden werden. Somit würde das Erzeugen des \textit{Docker Images} für den \textit{Proxy} Dienst überflüssig. Auch bei Änderungen müsste im äußersten Fall der Dienst nur neugestartet werden oder im Fall von \textit{Nginx} würde ein Steuersignal zum erneuten Laden der Konfigurationsdateien ausreichen. Für den produktiven Betrieb sollten jedoch \textit{Bind Mounts} nicht genutzt werden, um die Konfigurationsdatei vor Veränderung zu schützen. Zur Laufzeit von \textit{Docker Compose} werden die Abhängigkeiten zu den anderen Diensten wie z.B. dem \textit{Play Server} über \textit{Docker Links} aufgelöst. Zusätzlich wird noch der Treiber für das Logging der Ausgaben deaktiviert. \textit{Docker Compose} ermöglich es jedoch zur Laufzeit die Logs gezielt auf einen einzigen oder mehrere ausgewählte Dienste zu filtern und somit ist eine Deaktivierung des Logging Treibers nicht notwendig.

\subsection{Couchbase Server und Sync Gateway}

Beim dem nächsten Dienst, dem \textit{Couchbase Server}, sieht die Konfiguration mit extra \textit{Dockerfile} ähnlich wie beim \textit{Proxy} Dienst aus. Beim Verweis auf \textit{Docker Image} des \textit{Couchbase Servers} wird keine Version angegeben und somit wird immer die aktuellste Enterprise Variante des \textit{Couchbase Servers} bei der ersten Ausführung bezogen. Ein Wechsel auf die Community Variante mit der genauen \textit{Couchbase Server} Version würde sich hier anbieten. Die Nutzung einer \textit{Alpine} Variante ist aktuell nicht möglich, da offiziell eine solche Variante von Couchbase nicht angeboten wird und auch zukünftig nicht geplant ist \cite{couchbase_alpine}. Auch scheint eine \textit{Alpine} Variante kein triviales Unterfangen zu sein, da bis zum aktuellen Zeitpunkt noch kein Image mit dieser Variante aus der Community zu Verfügung gestellt wurde. In diesem \textit{Dockerfile} wird anstelle einer Konfigurationsdatei wie im \textit{Proxy} Dienst jedoch ein Shell-Script in das eigene \textit{Docker Image} kopiert. Dies ist notwendig damit die Datenbank und ein Benutzer mit einem Passwort initialisiert wird, da das offizielle Image diese Funktion von sich aus nicht bietet.

Anstelle das Ganze mit einem \textit{Dockerfile} zu lösen, kann der ursprüngliche \textit{ENTRYPOINT} in \textit{Docker Compose} z.B. durch einen \textit{Bind Mount} überschrieben werden und die einhergehende Anpassung des Pfads zum \textit{ENTRYPOINT} könnte ganz weggelassen werden. Damit lässt sich auch beim Dienst für den \textit{Couchbase Server} der zusätzliche Build Vorgang für das \textit{Docker Image} umgehen. Des Weiteren ist eine Datenbank von Natur aus zustandsbehaftet und auch der \textit{Couchbase Server} schreibt die Daten der Datenbanken in ein bestimmtes Verzeichnis. Aus diesem Grund wird in \textit{Docker Compose} zur Persistierung das Verzeichnis mit den Daten auf das Hostsystem gemapped. Dabei wird dieses Verzeichnis global in das Hauptverzeichnis gemapped, anstelle es auf das lokale Verzeichnis von \textit{Docker Compose} zu mappen. Auch einige Ports des \textit{Couchbase Servers} sind für den Zugriff vom Hostsystem und extern freigegeben. Für den Betrieb von \textit{Zeta} ist dies nicht notwendig, da die Zugriffe von den verschiedenen Diensten über die internen Links von \textit{Docker} erfolgen und der Rest über den Reverse-Proxy erreichbar ist. Zum Debuggen oder zu Testzwecken kann ein Zugriff über das Hostsystem erfolgen, aber ein externer Zugriff sollte aufgrund des Prinzip Secure-by-Default untersagt sein, um nicht schlussendlich diese Konfiguration für den produktiven Betrieb zu übernehmen.

Beim \textit{Database} Dienst wird für das \textit{Couchbase Sync Gateway} auch wieder ein \textit{Dockerfile} mit einer Konfigurationsdatei, einem eigenen \textit{ENTRYPOINT} und der lokalen sowie externen Freigabe einiger Ports per \textit{Docker Compose} gearbeitet. Dazu sei zu erwähnen, dass beim \textit{Docker Image} die Community Variante mit einer spezifischen Version genutzt wird. Auch beim \textit{Sync Gateway} wird keine \textit{Alpine} Variante, weder von offizieller Seite, noch aus der Community angeboten. Außerdem werden die freigegebenen Ports aufgrund der Links per \textit{Docker Compose} nicht für den Betrieb von \textit{Zeta} benötigt und der Zugriff sollte entsprechend auf das Hostsystem begrenzt sein. Auch um einen externen Zugriff auf den Port der Admin \ac{rest} \ac{api} in jedem Fall zu unterbinden.

\subsection{Node.js Server der Webapp mit Bower}

Der \textit{Webapp} Dienst nutzt auch ein eigens erstelltes \textit{Dockerfile} und ist ein \ac{npm} Projekt für \textit{Node.js}. Dabei wird auf das offizielle \textit{Node.js} Image mit einer bestimmten Version aufgebaut. Bei der Version wird entsprechend des Semantic Versionings aber nur ein Minor Release angeben und nicht einschließlich des Patch Release \cite{semver}. Für das offizielle \textit{Node.js} Image steht auch eine minimalisierte \textit{Alpine} Variante zur Verfügung. Im weiteren Verlauf des \textit{Dockerfiles} geht es primär um den Download der über \ac{npm} und \textit{Bower} definierten Abhängigkeiten. Zum einen sollte statt \ac{npm} der \textit{Yarn} Pakete Manager genutzt werden. \textit{Yarn} nutzt die offiziellen Server von \ac{npm} und unterstützt auch dasselbe Format zum Definieren der Abhängigkeiten. Aber der Prozess des Auflösen und Downloaden der Abhängigkeiten wurde gegenüber \ac{npm} stark verbessert und \textit{Yarn} steht inzwischen sogar in der \textit{Alpine} Variante des \textit{Node.js} Images zur Verfügung. Zum anderen verursacht die Nutzung von \textit{Bower} als weiterer Pakete Manager aufgrund der \textit{Polymer} Bibliothek eine höhere Komplexität. Inzwischen sind immer mehr Pakete direkt per \ac{npm} verfügbar und die Nutzung eines weiteren Paket Managers speziell für webbasierte Bibliotheken oder Frameworks ist immer seltener von Nöten. Auch das \textit{Polymer} Projekt hat diesen Veränderung inzwischen wahrgenommen und wird in naher Zukunft mit der nächsten Major Version von einem \textit{Bower} Projekt auf ein \ac{npm} Projekt wechseln \cite{polymer3_preview}. Somit lässt sich hier auch die Komplexität nach dem \ac{kiss} Prinzip weiter verringern. Eine alternativer Ansatz der jetzt schon möglich ist, ist das Auflösen per \textit{Bower} über einen \textit{Hook} in den regulären Installationsprozess von \ac{npm} bzw. \textit{Yarn} zu integrieren. Des Weiteren sind die genauen Befehle und ihre Reihenfolge der Pakete Manager nur im \textit{Dockerfile} definiert und es muss für jede Änderung an den Abhängigkeiten erneut ein \textit{Docker Image} erzeugt werden. Durch die Lösung mit dem \textit{Hook} kann nach dem \ac{soc} Prinzip das Auflösen der Abhängigkeiten zurück in das \ac{npm} Projekt verlagert werden und nun sogar ohne die Nutzung von \textit{Docker} ausgeführt werden. Somit müsste auch das Image während der Entwicklung von \textit{Zeta} nicht bei jeder Änderung an den Abhängigkeiten neu erzeugt werden und könnte durch einen Neustart des \textit{Webapp} Diensts erfolgen. Schlussendlich werden im \textit{Dockerfile} noch alle Dateien der \textit{Webapp} in das Image kopiert.

Zur Laufzeit wird dann per \textit{Docker Compose} erneut der Inhalt der \textit{Webapp} in den \textit{Docker Container} per \textit{Bind Mount} gemapped. Das Mapping der \textit{Webapp} ist notwendig um nicht nach jeder Dateiänderung das Image neu zu erzeugen und der Einsatz von \textit{Browsersync} wäre ansonsten überflüssig. Des Weiteren werden sogenannte \textit{Docker Volumes} auf die Verzeichnisse mit den heruntergeladenen Paketen der Abhängigkeiten von \ac{npm} und \textit{Bower} gemountet. Dabei ist bei der aktuellen Konfiguration zu beachten, dass beim ersten Start des \textit{Webapp} Diensts das Volume noch gänzlich leer ist und anders als beim \textit{Bind Mount} wird der Inhalt des Verzeichnis aus dem Container in das Volume geschrieben \cite{docker_storage_tips}. Entsprechend werden hier die Daten der Pakete in das Volume geschrieben. Bei einem zweiten Start des \textit{Webapp} Diensts ist das Volume nicht mehr leer und verhält sich nun wie ein \textit{Bind Mount}. Der Container kann über das Zielverzeichnis auf den Inhalt des Volumes oder \textit{Bind Mounts} zugreifen. Etwaiger Inhalt des Zielverzeichnisses, der z.B. beim Erzeugen des Image erstellt wurde, wird nicht überschrieben, aber wird vom Inhalt des Volumes oder \textit{Bind Mounts} überlagert. Dies führt z.B. zu dem Effekt beim \textit{Webapp} Dienst, dass zu Beginn alles normal funktioniert. Aber sobald aufgrund von Änderungen an den Abhängigkeiten das Image neu erzeugt wird, wird zur Laufzeit des Containers der Inhalt aus dem Volume mit den alten Abhängigkeiten genutzt und nicht die veränderten Abhängigkeiten aus dem Image. Des Weiteren werden zur Laufzeit sämtliche Ports zusätzlich extern freigegeben. Dies ist zum Betrieb von \textit{Zeta} nicht notwendig und sollte wie schon zuvor auf das Hostsystem begrenzt sein. Falls nicht zwingend notwendig, sollten diese Port-Freigaben gänzlich entfernt werden. Zusätzlich wird zur Laufzeit ein \textit{Workingdir} gesetzt und mit den zuvor erwähnten Änderung beim Auflösen der Anhängigkeiten ist das eigene \textit{Docker Image} nicht länger notwendig. Somit ist der aktuelle Build Vorgang für das \textit{Docker Image} des \textit{Webapp} Diensts überflüssig. Weiterhin wird auch der Treiber für das Logging der Standardausgabe des Containers wieder innerhalb von \textit{Docker Compose} deaktiviert.

\subsection{MongoDB Dienst}

Für den \textit{Mongodb} Dienst wird das offiziele \textit{Docker Image} und kein eigenes Image muss noch zusätzlich erzeugen werden. Zusätzlich wird auch eine Version für das Image mit angegeben. Jedoch ist die angegebene Version nur ein Major Release und kein Patch Release. Von offizieler Seite gibt es keine \textit{Alpine} Variante für das \textit{Mongo} Image. Aber aus der Community werden einige \textit{Docker Images} auf Basis von \textit{Alpine} für \textit{Mongo} bereitgestellt. Jedoch kommt keines dieser Images an den Funktionsumfang und die Qualität des offiziellen \textit{Mongo} Image heran. Entweder ist der Bezug einer bestimmten Version des \textit{Mongo} Server nicht möglich oder die Images haben keine automatische Einrichtung eines Benutzers mit Passwort. Aktuell wird die Funktion zur Einrichtung eines Benutzers mit Passwort nicht für den \textit{Mongodb} Dienst in \textit{Zeta} genutzt und somit ist Standardmäßig ein anonymer Zugriff auf die Datenbank möglich.  Das Verzeichnis, in dem der \textit{Mongo} Server die Daten schreibt, wird per \textit{Bind Mount} auf das Hostsystem gemapped. Jedoch wird auch diesmal wieder dafür ein Verzeichnis im globalen Stammverzeichnis genutzt und nicht ein lokales Verzeichnis. Desweiteren wird auch der Port extern freigegeben. Dies ist zum Betrieb von \textit{Zeta} nicht notwendig und sollte nach dem Prinzip Secure-by-Default wie zuvor auf das Hostsystem begrenzt sein. Schlussendlich ist auch bei diesem Dienst der Treiber für das Logging deaktiviert worden und sollte aus demselben Grund wie zuvor die Standard Einstellung benutzen.

\subsection{Images per sbt-native-packager}

Die restlichen Dienste in der \textit{Docker Compose} Konfiguration wie der \textit{Play Server} oder \textit{Workers} bauen auf eigene \textit{Docker Images} auf. Dabei werden diese \textit{Docker Images} nicht von \textit{Docker Compose} erzeugt, sondern müssen händisch vor dem ersten Start von \textit{Docker Compose} über das zuvor erwähnte Setupscript aus dem Programmcode~\ref{lst:ZETA_SETUP} auf Seite~\pageref{lst:ZETA_SETUP} erzeugt werden. Die Erzeugung der Images erfolgt pro \ac{sbt} Unterprojekt über den Task \textit{docker:publishLocal} des \textit{sbt-native-packager} Plugins. Das Plugin muss innerhalb von \ac{sbt} in den entsprechenden Unterprojekten per \textit{JavaAppPackaging} aktiviert sein. Jedoch wie in der List der genutzten \ac{sbt} Plugins aus der Tabelle~\ref{tab:ZETA_SBT_OLD} auf Seite~\pageref{tab:ZETA_SBT_OLD} zusehen ist, ist der \textit{sbt-native-packager} nicht in \textit{Zeta} enthalten. Bei einer genaueren Analyse der genutzten \ac{sbt} Plugins stellt sich heraus, dass der \textit{sbt-native-packager} eine Abgängigkeit des \ac{sbt} Plugin für das \textit{Play Framework} ist \cite{play_sbt_plugin_packager}. Nun nutzt \textit{Zeta} die Funktionalität des \textit{sbt-native-packager} außerhalb des Unterprojekts für den \textit{Play Server} und sollte aus diesem Grund auch explizit bei den genutzten \ac{sbt} Plugins aufgeführt werden.

Des Weiteren müssen während der Entwicklung von \textit{Zeta} nach jeder Änderungen am Programmcode, z.B. am \textit{Play Server} oder dem \textit{Workers} Dienst, die \textit{Docker Images} erneut erzeugt werden, da in den per \ac{sbt} erzeugten \textit{Docker Images} nur noch die Java Archive mit den kompilierten Scala Dateien enthalten sind. Aufgrund des fehlenden Build-Systems innerhalb der Images kann auch nicht zur Laufzeit der Programmcode in den \textit{Docker Container} gemapped werden. Die Erzeugung der Images per \ac{sbt} ist auf den Einsatz in einem Produktivsystem optimiert, aber erzeugt zusätzlichen Overhead bei Nutzung als Entwicklungssystem.

Zusätzlich sollte beim Einsatz von \textit{Docker Compose} ein Projekt nach dem \ac{kiss} Prinzip aufgesetzt werden. Entsprechend sollte beim Aufsetzen eines Entwicklersystems für \textit{Zeta} nur das Auschecken der aktuellen Version aus der Versionsverwaltung und das anschließende Ausführen von \textit{Docker Compose} notwendig sein. Ein alternativer Ansatz ist die Ausführung der Unterprojekte über den \ac{sbt} Launcher. Im Fall des \textit{Play Servers} wird das \textit{Play Framework} bei der Ausführung über den Launcher im Entwicklungsmodus und nicht wie in der aktuellen Lösung im Produktivmodus ausgeführt. Dabei unterstützt das \textit{Play Framework} den Entwickler durch automatisches Erkennen von geänderten Dateien und führt einen Incremental Build Vorgang aus. Die Erkennung von Änderungen wird bei jedem Aufruf von z.B. eines \ac{html} Dokuments oder einem Asset im Hintergrund ausgeführt und entstandene Fehler beim Build Vorgang werden dem Entwickler direkt als Webseite zurückgegeben. Damit benötigt der \textit{Play Server} gegenüber den weiteren Diensten wie z.B. dem \textit{Workers} Dienst nicht einmal einen Neustart um den Build Vorgang auszuführen.

\subsection{Container abhängige Konfiguration}

Die verschiedenen Dienste wie der \textit{Play Server} oder der \textit{Workers} Dienst nutzen in der \textit{Docker Compose} Konfiguration entweder Umgebungsvariablen oder Übergabeparameter um \textit{Docker} spezifische Einstellung wie z.B. Hostnamen zu den anderen \textit{Docker Container} festzulegen. Dies wird aber bei weitem nicht für alle Docker spezifischen Einstellungen gemacht. Einstellungen wie Hostnamen oder Zugangsdaten für die verschiedenen Datenbank Dienste sind in Konfigurationsdateien oder Scala Klassen hartcodiert \cite{zeta_docker_mongodb,zeta_docker_couchbase_admin,zeta_docker_couchbase_public,zeta_docker_play_seeds,zeta_docker_remote_api}. Die Scala Projekte sollten unabhängig von \textit{Docker} konfiguriert sein, um keine Bindung an \textit{Docker} zu verursachen. Die Information wie z.B. Hostnamen zum Zusammenspiel der verschiedenen Dienste sollte von außen an die Dienste herangetragen werden. Dies ermöglich auch z.B. die Umbenennung eines Dienst in der \textit{Docker Compose} Konfiguration ohne das zusätzlich eine einzige Zeile am Programmcode eines der Dienstes geändert werden muss.

Des Weiteren ist die Nutzung von Umgebungsvariablen und Programmparametern in \textit{Docker Compose} für sicherheitsrelevante Informationen wie z.B. Zugangsdaten in einem lokalen Entwicklungssystem ausreichend. In einem Produktivsystem sollte die Nutzung derselben für sicherheitsrelevante Information durch \textit{Docker Secrets} ersetzt werden. Umgebungsvariablen und auch Programmparameter können von Benutzern mit den entsprechenden Rechten systemweit ausgelesen werden. Die \textit{Docker Secret} Funktion gehört zum \textit{Docker Swarm} und stellt nur dem freigegebenen Container innerhalb eines \textit{Docker Cluster} die Information über einen sicheren Kanal zur Verfügung.

\subsection{Flexible Skalierung der Dienste}

Mit dem \textit{B1} und \textit{B2} Dienst sind erste Ansätze zur Skalierung der Dienste in der \textit{Docker Compose} Konfiguration vorhanden. Jedoch ist dieser Ansatz mit sehr viel Aufwand bei der Skalierung verbunden, da für einen weiteren Master oder Slave ein neuer Dienst angelegt werden muss und die Konfiguration der bestehenden Dienste angepasst werden muss. \textit{Docker Compose} unterstützt eine Funktion namens Replikation. Dadurch kann in der Konfiguration angegeben werden wie oft ein bestimmter Dienst von \textit{Docker Compose} gestartet werden soll. Als Beispiel kann z.B. ein Dienst namens Master mit fünf Replikationen in \textit{Docker Compose} angelegt werden. Zur Laufzeit von \textit{Docker Compose} sind die Dienste \textit{zeta\_master\_1} bis \textit{zeta\_master\_5} vorhanden. Auf den ersten Blick ist weiterhin die Anpassung der anderen Dienste wie z.B. dem \textit{workers} Dienst notwendig. Jedoch liefert \textit{Docker} per DNS Loadbalancing unter der Adresse \textit{zeta\_master} die IP Adressen aller fünf laufenden Master Dienste. Beim Joinen der Nodes in den \textit{Akka Cluster} müssten alle Adressen und nicht nur die erste Adresse per DNS aufgelöst werden. Dies ermöglicht z.B. in einen Produktivsystem einen skalierbaren \textit{Akka Cluser} ohne zusätzlichen Konfigurationsaufwand. Über einen \textit{Docker Swarm} wäre zusätzlich eine Skalierung über mehrere Computer ohne weiteren Konfigurationsaufwand möglich. Über diesen Mechanismus ist auch eine transparente Skalierung des \textit{Play Server} über mehrere Instanzen möglich.  

Beim Einsatz in einem Entwicklersystem spielt nicht mehr eine möglichst große Skalierung von \textit{Zeta} die Hauptrolle, sondern im Gegensatz dazu soll \textit{Zeta} mit minimalen Overhead und geringem Ressourcenverbrauch ausgeführt werden. Gleichzeitig sollen auch automatische Build Vorgänge und Dienste im speziellen Modus für Entwickler unterstützt werden. Zu diesem Zweck macht es Sinn den einen oder anderen Dienst falls möglich zusammen zu legen. Die Dienste \textit{B1}, \textit{B2}, \textit{developers} und \textit{workers} werden alle aus demselben \textit{Docker Image} bzw. \ac{sbt} Unterprojekt gestartet. Diese Dienste könnten durch das Hinzufügen spezifischer Programmparameter und der Nutzung des \textit{Akka Clusters} sogar innerhalb eines Prozesses derselben JVM gestartet werden. Diese gewonnene Flexibilität ermöglicht zusätzliche Szenarien bei der Skalierung von \textit{Zeta}.

\subsection{Datenzugriffe des Generator Containers}

Neben dem Image für den \textit{Play Server} und den Diensten des \textit{Akka Clusters} werden für den \textit{ScalaFilter} und die vier weiteren \ac{sbt} Unterprojekte der Generatoren noch jeweils ein \textit{Docker Image} erzeugt. Aus dem Image werden zur Laufzeit von \textit{Zeta} neue Container gestartet. Dabei treten diese Container dem privaten \textit{Docker Compose} Netzwerk bei, um einen Zugriff auf den \textit{Play Server} und das \textit{Couchbase Sync Gateway} zu ermöglichen. Zuerst einmal ist der Name des \textit{Docker Compose} Netzwerk in der \textit{DockerWorkExecutor} Klasse hartcodiert und dabei ist der Name abhängig von der \textit{Docker Compose} Konfiguration. Diese Information sollte aufgrund des \ac{soc} Prinzipes von außerhalb an den \textit{workers} Dienst herangetragen werden. Zusätzlich ist der Beitritt zum \textit{Docker Compose} Netzwerk nicht notwendig, da zum einen der \textit{Play Server} und das \textit{Couchbase Sync Gateway} extern über den \textit{Proxy} Dienst erreichbar sind.

Das eigentliche Problem bei dieser Lösung ist jedoch der direkte Zugriff auf das \textit{Couchbase Sync Gateway}. Die Generatoren können ohne Authentifizierung über die Admin \ac{rest} \ac{api} auf die Datenbank zugreifen. Über den \textit{Proxy} Dienst wäre der Zugriff auf die Public \ac{rest} \ac{api} des \textit{Synx Gateways} begrenzt und durch eine Authentifizierung geschützt. In Teilen wird die Public \ac{rest} \ac{api} schon in den Generatoren verwendet und zur Authentifizierung kommt ein Cookie mit einer Session ID für das \textit{Sync Gateway} zum Einsatz. Die Session ID wird hierbei beim Starten des Containers als Programmparameter übergeben.

Inneralb der Generatoren werden die \ac{m2t} Transformationsregeln aus der Datenbank abgefragt. Die Transformationregeln bestehen aus Scala Programmcode und können flexibel eingesetzt werden. Aus diesen Benutzer definierten Transformationsregeln wird ein Transformer kompiliert und ausgeführt. Neben den Daten der Model Instanz bekommt der Transformer eine \ac{api} mit Vollzugriff auf die Datenbank übergeben. Damit kann ein Benutzer über die Transformationsregeln alle Daten auslesen, verändern und löschen. Selbst ohne die Übergabe der \ac{api} hat der Benutzer über die Transformationsregeln weiterhin Vollzugriff auf die Datenbank, da alle an den Container übergebenen Informationen wie Programmparameter, Umgebungsvariablen oder selbst \textit{Docker Secrets} ausgelesen werden können. Der Transformer läuft innerhalb derselben \ac{jvm} und hat auf alle Klassen des Generators Zugriff und kann sich zur Not seinen eigenen Client für das \textit{Sync Gateway} erstellen. Die Integrität von \textit{Zeta} ist durch diese Lösung erheblich geschwächt und sollte grundlegend überdacht werden.

Sämtliche Datenzugriffe der Generatoren sollten wie ein öffentlicher Datenzugriff gehandhabt werden und sollten nach den Prinzipien Least-Privilege und Separation-of-Duties erfolgen. Ein möglicher Lösungsansatz für das Problem wäre eine Partitionierung der Daten in eine Datenbank pro Benutzer. Wobei zusätzlich noch bestimmte Daten für alle Benutzer zugänglich sein müssen. Dies erzeugt zum einen deutlich höheren Verwaltungsaufwand auf Seiten der Datenbankbenutzer und teils weitere schwer absehbare Zusatzaufwände. Desweiten kann der Benutzer aufgrund des frei wählbaren Schemas einer \textit{Couchbase Datenbank} auch Daten in einem für das Back-End ungültigen Schema erzeugen und so weitere schwer vorhersehbare Probleme verursachen.  Ein weiterer Lösungsansatz könnte durch einen indirekten Zugriff auf die Datenbank realisiert werden. Dabei werden die Generatoren von den Datenbank getrennt und  die Zugriffe erfolgen z.B. über die \ac{rest} \ac{api} des \textit{Play Server}. Innerhalb des \textit{Play Server} kann der Datenzugriff für den Generator nur auf die notwendigsten Daten begrenzt werden. Dabei kann eine Session ID für den \textit{Play Server} an die Generatoren übergeben werden, da auch der Transformer nur auf die eingeschränkten Daten eines einzigen Benutzers Zugriff hat. Ein eher klassischer Ansatz wäre es, den Datenaustausch per Interprozesskommunikation über den \textit{DockerWorkExecutor} zu realisieren. Weitere Lösungsansätze sind möglich, aber der entscheidende Faktor ist die Sicherstellung einer streng regulierten und kontrollieren Kommunikation mit dem restlichen Back-End von \textit{Zeta}.

\subsection{Datenbank Fixtures für Generatoren}

Neben den Images für die \textit{Docker Compose} Dienste und den Generatoren wird am Ende des Setupscripts noch ein weiteres \textit{Docker Image} erzeugt. Das \textit{modigen:data} getaufte Image hat zur Aufgabe, während der Ausführung als \textit{Docker Container}, initial einige notwendige Daten für die Generatoren in die \textit{Couchbase Datenbank} zu schreiben. Das ganze muss also zusätzlich zum Setupscript ausgeführt werden, aber erst nachdem \textit{Zeta} durch \textit{Docker Compose} gestartet wurde. Die exakte Dokumentation des Befehls zur Ausführung des Containers ist nur in der Readme enthalten und hat als einzigen Parameter das private Docker Compose Netzwerk von \textit{Zeta}. Die enwickelte Lösung enthält dabei hartcodierte Zugangsdaten zur Public \ac{rest} \ac{api} des \textit{Couchbase Sync Gateways} und hat keinerlei Möglichkeit diese z.B. über Programmparameter oder Umgebungsvariablen von außen zu konfigurieren \cite{zeta_docker_data}. 

Die Lösung baut auf \textit{Node.js} auf und besteht primär aus einem Script zur Erstellung von Einträgen für die vier Generator Images in der Datenbank. Weder in \textit{Webapp} noch in der Generator Verwaltung per \textit{Akka Cluster} ist irgendeine Verwaltung der Generator Images vorhanden. Für die Einträge der Generator Images sind nur lesende Zugriffe vorhanden. Diese Lösung ist schon von der grundsätzlichen Konzeption schwierig. Nach dem \ac{kiss} und \ac{dry} Prinzip wäre eine Lösung in einem der bestehenden Dienste zu bevorzugen. Auch nach dem Prinzip zur Modularisierung wie z.B. dem Prinzip der Hohen Kohäsion würde die Lösung eher in einer der Dienste des \textit{Akka Cluster} zur Verwaltung der Generatoren gehören und nicht in ein eigenes Module. Zusätzlich rechtfertigt der erhöhte Implementierungs-, Build, und Einrichtungsaufwand nicht die gewählte Lösung. Dabei kann das ganze aufgrund der Lesezugriffe mit minimaler Komplexität über eine List innerhalb der Konfiguration oder in diesem Fall sogar hartcodiert gelöst werden. Zusätzlich ist für die aktuelle Lösung auch der Zugriff auf die Public \ac{rest} \ac{api} des \textit{Couchbase Sync Gateways} über das private \textit{Docker Composer} Netzwerk nicht einmal notwendig. Die Public \ac{rest} \ac{api} ist über den \textit{Proxy} Dienst auch außerhalb des Netzwerks erreichbar.

\subsection{Fazit}

Abschließend geht die Nutzung einer Containervirtualisierung wie \textit{Docker} bzw. \textit{Docker Compose} für die verschiedenen Dienste von \textit{Zeta} in die richtige Richtung. \textit{Docker Compose} ermöglicht eine reproduzierbare Laufzeitumgebung für den Einsatz vom Entwickler- bis zum Produktivsystem. Dabei kann dank \textit{Docker-for-Windows} \textit{Zeta} mit einem etwas größeren initialen Einrichtungsaufwand bei \textit{Docker} sogar auf Windows ausgeführt werden. Zusätzlich kann über die \textit{Docker Compose} Konfiguration ein schneller Überblick der Abhängigkeiten und der Beziehungen zwischen den Diensten erfolgen. Desweiten ermöglicht \textit{Docker Compose} einem Entwickler, in kurzer Zeit sein Entwicklungssystem lauffähig zu haben. Jedoch bringt die aktuelle \textit{Docker Compose} Umgebung noch einigen zusätzlichen Overhead mit sich. Dieser Overhead kommt zum einen beim ersten Start von \textit{Docker Compose} zum Tragen, aber zusätzlich auch bei der Weiterentwicklung von \textit{Zeta}. \textit{Docker Compose} bietet eine kompakte, aber sehr mächtige Syntax, die einige Einarbeitung in die Containervirtualisierung voraussetzt. Eine Vereinfachung der Konfiguration und ein stärkerer Fokus auf eine Umgebung für ein Entwicklersystem könnte den Nutzen hinter dem Einsatz von \textit{Docker Compose} für \textit{Zeta} noch weiter steigern.

\section {Scala Projekt}

Das Back-End von \textit{Zeta} besteht wie in Abbildung~\ref{fig:ZETA_ARCH_OLD} auf Seite~\pageref{fig:ZETA_ARCH_OLD} zu sehen, aus einer Reihe von Diensten. Neben den unterschiedlichen Datenbank Diensten und dem \textit{Proxy} Dienst sind die weiteren Dienste wie der \textit{Play Server}, der \textit{B1} (Master), der \textit{workers}, der \textit{developers} Dienst und auch die Generatoren eigens für \textit{Zeta} entwickelt worden. Diese Dienste sind mit Scala realisiert worden und befinden sich als ein gemeinsames Projekt im \textit{api} Verzeichnis. Als Build-Tool wird für dieses Scala Projekt \ac{sbt} verwendet. Neben \ac{sbt} können auch andere Build-Tools wie z.B. \textit{Maven}, \textit{Gradle} oder auch \textit{Ant} für Scala verwendet werden. Jedoch wurden diese Build-Tools primär für Java Projekte geschrieben, während \ac{sbt} primär für Scala Projekte geschrieben worden ist. Des Weiteren ist \ac{sbt} das von Scala selbst favorisierte Build-Tool zur Programmierung mit Scala und entsprechend nutzen viele Scala Projekte \ac{sbt} als ihr Build-Tool \cite{scala_getting_started}. Aufgrund des \textit{Play Frameworks} muss bei \textit{Zeta} jedoch sogar \ac{sbt} zwingend verwendet werden. Im weiteren Verlauf dieses Abschnitts wird näher auf die einzelnen Unterprojekte des Scala Projekts eingegangen. Dabei werden auf die grundlegenden Lösungsansätze wie Entwurfsmuster eingegangen.

\subsection{Modularisierung}
\label{subsec:REVIEW_MODULES}

Das Scala Projekt ist über \ac{sbt} als Multi-Project Build konfiguriert und ist wie in Abbildung~\ref{fig:ZETA_SBT_OLD} auf Seite~\pageref{fig:ZETA_SBT_OLD} zu sehen in mehrere Unterprojekte aufgeteilt. Die Größe der verschiedenen Projekte kann in der Tabelle~\ref{tab:ZETA_METRICS_LOC_OLD} auf Seite~\pageref{tab:ZETA_METRICS_LOC_OLD} eingesehen werden. Dabei enthält die Tabelle die Gesamtanzahl an Zeilen der jeweiligen Dateien, der \ac{sloc} und der \ac{cloc}. In diesem Zusammenhang ist die \ac{sloc} definiert als Anzahl an Zeilen pro Datei ohne Leer- und Kommentarzeilen. Zusätzlich enthält die Tabelle~\ref{tab:ZETA_METRICS_OLD} auf Seite~\pageref{tab:ZETA_METRICS_OLD} weitere allgemeine Metriken über die \ac{sbt} Unterprojekte. Dabei ist zu beachten, dass die Klassen Metrik die Anzahl an Class und Object Definitionen, aber auch die Case und Abstract Varianten enthält. Die zugrundeliegenden Daten für die Tabellen sind über das Programm \textit{cloc} in der Version 1.60, \textit{Sonarqube} in der Version 2.1.0 und durch das Bash-Script im Anhang~\ref{subsec:APPENDIX_LISTINGS_STATS_OLD} auf Seite~\pageref{subsec:APPENDIX_LISTINGS_STATS_OLD} mit den Aufrufen aus Anhang~\ref{subsec:APPENDIX_LISTINGS_STATS_EXECS} auf Seite~\pageref{subsec:APPENDIX_LISTINGS_STATS_EXECS} ermittelt worden \cite{cloc}.

\begin{table}[ht]
    \smallskip
    \centering
    \begin{tabular}{| l | r | r | r |}
    \hline
    \bf Projekt & \bf Gesamt & \bf SLOC & \bf CLOC \\ \hline
    backend & 2257 & 1712 & 195 \\ \hline
    client & 1454 & 1272 & 52 \\ \hline
    common & 3367 & 2158 & 698 \\ \hline
    scalaFilter & 125 & 104 & 0 \\ \hline
    basicGenerator & 99 & 61 & 27 \\ \hline
    fileGenerator & 138 & 93 & 27 \\ \hline
    remoteGenerator & 152 & 95 & 33 \\ \hline
    specificGenerator & 158 & 103 & 36 \\ \hline
    template & 302 & 210 & 60 \\ \hline
    metaModelRelease & 65 & 52 & 0 \\ \hline
    server & 10540 & 7377 & 1838 \\ \hline
    shared & 32 & 22 & 0 \\ \hline
    \bf Gesamt & \bf 18689 & \bf 13259 & \bf 2966 \\ \hline
    \end{tabular}
    \caption{Ausganszustand - \ac{loc} der Scala Dateien \cite{analys_old_directory}}
    \label{tab:ZETA_METRICS_LOC_OLD}
\end{table}

Aus der Übersicht der verschiedenen \ac{sbt} Unterprojekte in der Abbildung~\ref{fig:ZETA_SBT_OLD} auf Seite~\pageref{fig:ZETA_SBT_OLD} ist erkennbar, dass die Unterprojekte keine zirkulären Abhängigkeiten untereinander haben. Dabei spielt das \textit{common} Unterprojekt eine zentrale Rolle bei den Unterprojekten und enthält einen Großteil der Case Klassen als Immutable Datencontainer. Diese werden z.B. für die Entities der Datenbanken benutzt. Zusätzlich ist auch die Implementierung zur Persistierung in der \textit{Couchbase Datenbanken} im \textit{common} enthalten. Weitere Klassen z.B. für den \textit{Akka Cluster} sind auch in \textit{common} und werden vom \textit{Play Server} und dem \textit{backend} Unterprojekt verwendet. Zusätzlich hängen alle Generator Unterprojekte auch von \textit{common} ab. Speziell die Generatoren Unterprojekte gehören zu Unterprojekte mit den wenigsten Dateien, Paketen und dem geringsten Programmcode. Die Modularisierung ist ein wichtiger Aspekt in der Software Programmierung. Dabei sollen nach dem Prinzip der Hohen Kohäsion Programmcode Bereiche die stark miteinander zusammenarbeiten, in einem Module zusammengefasst werden. Die Unterprojekte \textit{basicGenerator}, \textit{fileGenerator}, \textit{specificGenerator} und \textit{remoteGenerator} sind bis auf den Beispiel oder Demo Programmcode für Transformationensregeln nahezu identisch. Am meisten sticht hier der \textit{remoteGenerator} heraus, der einen erweiterten Aufruf mit Optionen ermöglicht. 

Des Weiteren ist die Frage, ob der Programmcode für die Transformationsregeln überhaupt in die Generator Projekte gehört. Beim Erstellen eines Generators über die \textit{Webapp} wird über einen zusätzlichen \textit{Docker Container} eines der Generator Projekte ausgeführt und innerhalb dieses Containers wird ein Generator mit einem der Beispiele als Transformationsregel in der Datenbank angelegt. Später beim Ausführen eines Generators wird auch wieder das entsprechende Generator Projekt in einem zusätzlichen \textit{Docker Container} ausgeführt und die Transformationsregeln werden zur Laufzeit kompiliert und ausgeführt. Die Erstellung und die Ausführung der Generatoren haben als gemeinsame Schnittstelle den Zugriff auf die Datenbank, aber untereinander besteht nur eine geringe Kohäsion. Zudem müssten die Beispiele der Transformationsregeln bei Änderungen am Modell geändert werden, während keine Änderung bei der Ausführung notwendig sind. Dies verletzt unter anderem das \ac{srp}. Die Erstellung der Generatoren könnte in einem davor gelagerten System wie z.B. dem \textit{backend} Dienst erfolgen. Des Weiteren zieht Modularisierung den meisten Nutzen aus der Wiederverwendung von z.B. Modulen bzw. Unterprojekten. Die Generator Projekte werden selbst aber von keinem anderen Unterprojekt genutzt. Eine Zusammenlegung dieser Projekte mit dem \textit{template} Unterprojekt reduziert auch die Anzahl zu erzeugender \textit{Docker Images} auf weniger als die Hälfte und der einhergehende Overhead wird zusätzlich reduzieren.

\ac{sbt} nutzt für ein Projekt bzw. Unterprojekt als Verzeichnis Struktur dieselbe Struktur wie ein \textit{Maven} Paket \cite{sbt_directories}. Der Programmcode befindet sich bei den Unterprojekten für \textit{Zeta} ähnlich wie bei Java Projekten im Verzeichnis \textit{src/main/scala} und die Tests befinden sich im Verzeichnis \textit{src/test/scala}. Bis auf das Unterprojekt für den \textit{Play Server} sind alle anderen Unterprojekte auch nach diesem Schema aufgebaut. Bei Projekten, die auf dem \textit{Play Framework} aufbauen, befindet sich der Programmcode wiederum im Verzeichnis namens \textit{app}, die Tests im Verzeichnis \textit{test} und Dateien für die Hauptkonfiguration wie \textit{application.conf} und \textit{routes} sind im Verzeichnis \textit{conf} \cite{play_framework_anatomy}. Das \textit{app} Verzeichnis besteht im Wesentlichen aus den Paketen \textit{controllers}, \textit{models} und \textit{views}. Wie schon an den Namen der Pakete zu sehen ist, folgt die Erstellung einer Anwendung mit dem \textit{Play Framework} dem \ac{mvc} Architektur Pattern \cite{play_main_concept}. Dabei enthalten die Pakete direkt die entsprechenden Klassen. Die Vorgehensweise, erst einmal keinen projektspezifischen Präfix für die Pakete zu verwenden, entspricht den Konventionen des \textit{Play Frameworks} \cite{play_framework_anatomy}. Obwohl seit Version 2 des \textit{Play Frameworks} auch Pakete mit Präfixen benutzt werden können.

Aus diesem Grund werden in \textit{Zeta} im gesamten Unterprojekt des \textit{Play Servers} nur Pakete ohne ein projektspezifisches Präfix benutzt. Zu Beginn bestand \textit{Zeta} nur aus dem \textit{Play Server} Projekt und umfasste aus diesem Grund den Großteil des Programmcodes. Mit der Einführung des \textit{client} mit dem \textit{shared} Projekt wurde \textit{Zeta} in mehrere \ac{sbt} Unterprojekte aufgeteilt. Jedoch wird neben dem \textit{Play Server} auch für die Pakete in den anderen Unterprojekten kein Präfix verwendet. Dies ist auch der Grund, wieso in der Abbildung~\ref{fig:ZETA_SBT_OLD} auf Seite~\pageref{fig:ZETA_SBT_OLD} die Verzeichnisse und nicht die Pakete bei den Unterprojekten stehen. Bei den Java Naming Konventionen wird vorausgesetzt, dass Pakete einen Präfix haben sollen \cite{java_package_naming}. Dabei soll der Präfix aus dem Namen des Unterprojekts bestehen und kann zusätzlich noch einen Firmen spezifischen Prefix enthalten. Diese Konvention macht es einfacher bei den Imports den Ursprung von einem Paket auf einen Blick zu sehen. Bei \textit{Zeta} z.B. existieren sowohl im Hauptverzeichnis des \textit{common} als auch im \textit{Play Server} Unterprojekt das Paket models. Bei einem Import einer Klasse aus diesem Paket ist das ursprüngliche Unterprojekt nicht erkennbar. Als weiteres Beispiel existiert im Hauptverzeichnis des \textit{common} Unterprojekt noch das \textit{cluster} Paket. Bei einem Import aus diesem Paket im \textit{backend} Unterprojekt wird auch die Unterscheidung zwischen einem Paket eines internen Unterprojekten und einem Pakets aus einer genutzten Bibliotheken für den \textit{Akka Cluster} immer schwieriger. Die Einführung eines Prefix in jedem Unterprojekt würde mehr Klarheit beim Aufbau und Strukturierung von \textit{Zeta} schaffen.

\begin{table}[ht]
    \smallskip
    \centering
    \begin{tabular}{| l | r | r | r | r | r |}
    \hline
    \bf Projekt & \bf Dateien & \bf Ordner & \bf Klassen & \bf Methoden & \bf Traits \\ \hline
    backend & 27 & 8 & 105 & 151 & 9 \\ \hline
    client & 14 & 4 & 50 & 514 & 16 \\ \hline
    common & 36 & 16 & 167 & 227 & 35 \\ \hline
    scalaFilter & 2 & 2 & 2 & 5 & 1 \\ \hline
    basicGenerator & 2 & 1 & 3 & 6 & 0 \\ \hline
    fileGenerator & 2 & 1 & 4 & 9 & 0 \\ \hline
    remoteGenerator & 2 & 1 & 8 & 11 & 0 \\ \hline
    specificGenerator & 2 & 1 & 3 & 12 & 0 \\ \hline
    template & 3 & 2 & 6 & 16 & 2 \\ \hline
    metaModelRelease & 1 & 1 & 2 & 1 & 0 \\ \hline
    server & 154 & 47 & 269 & 758 & 30 \\ \hline
    shared & 2 & 1 & 13 & 0 & 3 \\ \hline
    \bf Gesamt & \bf 247 & \bf 85 & \bf 632 & \bf 1710 & \bf 96 \\ \hline
    \end{tabular}
    \caption{Ausgangszustand - Metriken über Scala Typen \cite{analys_old_directory}}
    \label{tab:ZETA_METRICS_OLD}
\end{table}

Zur Auflösung der Abhängigkeiten innerhalb des \textit{Play Servers} z.B. für die Abhängigkeiten der Controller wird das \ac{di} Entwurfsmuster benutzt. Dabei setzt das \textit{Play Framework} auf das leichtgewichtige \ac{di} Framework von Google namens \textit{Guice} \cite{play_guice}. Die \textit{Guice} Module zum Auflösen der Abhängigkeiten, wie in Tabelle~\ref{tab:ZETA_PLAY_PACKAGES} auf Seite~\pageref{tab:ZETA_PLAY_PACKAGES} zusehen, werden im \textit{modules} Verzeichnis definiert und über den \textit{play.modules.enabled} Eintrag in der \textit{application.conf} automatisch vom \textit{Play Framework} zur Laufzeit initialisiert. Im \textit{Play Server} wird primäre das \ac{di} per Konstruktor verwendet und ist laut Martin Fowler auch die bevorzugte \ac{di} Variante \cite{depenency_injection}. Ansonsten wird zwar in den restlichen Unterprojekten zum Teil das \ac{di} Entwurfsmuster benutzt, jedoch nicht durchgängig wie z.B. beim \textit{SyncGatewaySession} im \textit{Mediator} Actor aus dem \textit{backend} oder in der Abstrakten Klasse \textit{Template} der Generator. Des Weiteren wird kein \ac{di} Framework wie z.b. \textit{Guice} bei den restlichen Unterprojekten eingesetzt.

\begin{table}[ht]
    \smallskip
    \centering
    \begin{tabular}{| l | l |}
    \hline
    \bf Paket & \bf Beschreibung \\ \hline
    assets & Javacript und \ac{css} für Code und grapgischen Editor \\ \hline
    controllers & Siehe \ac{mvc} Entwurfsmuster \\ \hline
    forms & Play Formular Definitionen \\ \hline
    generator & SprayParser und Models mit JavaScript Generatoren \\ \hline
    jobs & AuthToken Cleanup \\ \hline
    models & Actoren, Authentifizierung  \\ \hline
    modules & Google Guice \ac{di} Module  \\ \hline
    util & DataVis \\ \hline
    utils & Authentifizierung, Sonstiges\\ \hline
    views & Siehe \ac{mvc} Entwurfsmuster \\ \hline
    \end{tabular}
    \caption{Ausgangszustand - Play Server Pakete (app Verzeichnis)}
    \label{tab:ZETA_PLAY_PACKAGES}
\end{table}

\subsection{Qualitätssicherung}
\label{subsec:REVIEW_QA}

Ein wichtiger Aspekt bei der Software Entwicklung sind Technische Schulden. Technische Schulden ist ein Konzept in der Software Entwicklung, dass den späteren Aufwand durch eine jetzige schnell oder einfach realisierte Lösung gegenüber einem besseren Ansatz reflektiert \cite{technical_debt}. Technischen Schulden können die unterschiedlichsten Ursachen haben und können schon während der Design Phase ihren Ursprung haben. Zur Erkennung und Minimierung von Technischen Schulden sollten unteranderem Linter und automatische Unittests eingesetzt werden.

\bigskip
\begin{lstlisting}[caption={Weiter Tools zur Qualitaetssicherung},label={lst:SBT_LINTER_OLD}]
addSbtPlugin("com.github.mwz" % "sbt-sonar" % "1.3.0")
addSbtPlugin("org.scalastyle" %% "scalastyle-sbt-plugin" % "1.0.0")
addSbtPlugin("org.wartremover" % "sbt-wartremover" % "2.2.1")
addSbtPlugin("org.scoverage" % "sbt-scoverage" % "1.5.1")
\end{lstlisting}
\smallskip

\textit{Zeta} hat in seinem jetzigen Zustand nur Prüfungen über Flags im Scala Compiler über den \textit{scalacOptions} Einstellung aktiviert und setzt keine Tools wie z.B. \ac{sbt} Plugins als Linter ein \cite{zeta_api_built}. Stattdessen setzt \textit{Zeta} zur Einhaltung eines Style Guides auf die \ac{sbt} Erweiterung Scalariform, wie in der Tabelle~\ref{tab:ZETA_SBT_OLD} auf Seite~\pageref{tab:ZETA_SBT_OLD} zu sehen, zur automatischen Formatierung des Programmcodes. Technische Schulden können mit dieser Lösung aber nur bedingt abgebaut werden. Zu diesem Zweck sind alle \ac{sbt} Unterprojekte von \textit{Zeta} im Rahmen dieser Arbeit mit weitere Tools, wie im Quelltext~\ref{lst:SBT_LINTER_OLD} auf Seite~\pageref{lst:SBT_LINTER_OLD} zusehen, zur statischen Codeanalyse analysiert worden. Das Ergebnis dieser Analyse kann in der Tabelle~\ref{tab:ZETA_METRICS_LINT_OLD} auf Seite~\pageref{tab:ZETA_METRICS_LINT_OLD} pro \ac{sbt} Unterprojekt eingesehen werden. Als Linter kommt der \textit{Scalastyle} v1.0.0 und \textit{WartRemover} v2.2.1 zum Einsatz \cite{scalastyle,wartremover}. Die Hauptgründe für den Einsatz genau dieser Linter sind, dass diese Linter schon mehrere Jahre alt sind, es ermöglichen eigene Regeln zu definieren, mindestens einen \textit{v1} als Stable Release haben und, dass diese Linter immer noch gewartet werden. Ein Projekt gilt in diesem Rahmen als gewartet, wenn im letzten Monat noch Aktivität in Form eines Commits, Bearbeitung eines Tickets oder die Veröffentlichung eines Release stattgefunden hat. Neben diesen Lintern existiert auch z.B. noch \textit{scalafix}. \textit{scalafix} wird von der für Scala verantwortlichen Hochschule entwickelt. Dieser ermöglich neben dem Linten auch die automatische Korrektur von gefundenen Problemen. Zum einen ist diese Projekt noch recht jung und zum anderen kann \textit{scalafix} aufgrund einer Abhängigkeit nur in Scala 2.11.12 oder 2.12.4 ausgeführt werden \cite{scalafix_sbt1}.

\begin{table}[ht]
    \smallskip
    \centering
    \begin{tabular}{| l | r | r |}
    \hline
    \bf Projekt & \bf Compile with & \bf Scalastyle \\ 
    ~ & \bf Wartremover & (Fehl./Warn.) \\ 
    ~ & (Fehl./Warn.) & ~ \\ \hline
    backend & 0 / 79 & 69 / 239 \\ \hline
    client & 0 / 106 & 21 / 209  \\ \hline
    common & 0 / 87 & 67 / 233 \\ \hline
    scalaFilter & 0 / 11 & 2 / 8 \\ \hline
    basicGenerator & 0 / 5 & 2 / 11  \\ \hline
    fileGenerator & 0 / 0 & 2 / 12  \\ \hline
    remoteGenerator & 0 / 1 & 2 / 17 \\ \hline
    specificGenerator & 0 / 0 & 2 / 15 \\ \hline
    template & 0 / 3 & 5 / 17 \\ \hline
    metaModelRelease & 0 / 2 & 1 / 4 \\ \hline
    server & 0 / 830 & 264 / 2744 \\ \hline
    shared & 0 / 0 & 0 / 1\\ \hline
    \bf Gesamt & \bf 0 / 1124 & \bf 437 / 3510 \\ \hline
    \end{tabular}
    \caption{Ausganszustand - Ergebnisse der Linter Prüfungen \cite[Scalastyle]{analys_old}}
    \label{tab:ZETA_METRICS_LINT_OLD}
\end{table}

Über die Flags für den Scala Compiler wird weder eine aktuelle noch eine umfassende Beschreibung in der offiziellen Scala Dokumentation angeboten. Einzig für den auf der Kommandozeile ausführbaren und offiziellen Scala Compiler \textit{scalac} existiert im Archive der Scala 2.10 Dokumentation aus dem Jahr 2012 eine Man-Page \cite{scalac_man_page}. Alternativ können per \textit{scalac} die aktuellste Information über die Compiler Flags über mehrere unterschiedliche Parameter wie z.B. dem \textit{-help} Parameter ausgegeben werden. \textit{Zeta} verwendet für Scala aber \ac{sbt} und somit steht \textit{scalac} gar nicht zur Verfügung. Über \ac{sbt} lassen sich aber über die Einstellung \textit{scalacOptions} Parameter an den Scala Compiler übergeben. Ein Ausgabe dieser Parameter für die in \textit{Zeta} genutzte Scala 2.11.7 lässt sich im Anhang~\ref{subsec:APPENDIX_LISTINGS_SCALAC_FLAGS} auf Seite~\pageref{subsec:APPENDIX_LISTINGS_SCALAC_FLAGS} einsehen. \textit{Zeta} nutzt eine einstellige Anzahl an Compiler Flags und zusätzlich nicht über alle Unterprojekte dieselben. Aus diesem Grund werden nachfolgend die in \textit{Zeta} genutzten Compiler Flags kurz beschrieben und die in dem genutzten Unterprojekt aufgelistet.

\begin{itemize}
  \item \textbf{-deprecation:} Warnt vor der Nutzung von als Deprecated markierte \acp{api}. Ist bei \textit{Zeta} in allen Projekten bis auf \textit{common}, \textit{client} und \textit{shared} aktiviert.
  \item \textbf{-feature:} Warnt von Funktionalitäten, die explizit importiert werden sollten. Ist bei \textit{Zeta} in allen Projekten bis auf \textit{common}, \textit{client} und \textit{shared} aktiviert.
  \item \textbf{-unchecked:} Zusätzliche Warnung bei generiertem Programmcode der auf Annahmen basiert. Ist bei \textit{Zeta} in allen Projekten bis auf \textit{common}, \textit{client} und \textit{shared} aktiviert.
  \item \textbf{-Xlint:} Ist bei \textit{Zeta} nur im \textit{server} Projekt aktiviert und ist für die Aktivierung eine Reihe von zusätzlich empfohlenen Flags verantwortlich:

  \begin{multicols}{2}
  \begin{itemize}
    \item \textit{-Xlint:adapted-args}
    \item \textit{-Xlint:nullary-unit}
    \item \textit{-Xlint:inaccessible}
    \item \textit{-Xlint:nullary-override}
    \item \textit{-Xlint:infer-any}
    \item \textit{-Xlint:missing-interpolator}
    \item \textit{-Xlint:doc-detached}
    \item \textit{-Xlint:private-shadow}
    \item \textit{-Xlint:type-parameter-shadow}
    \item \textit{-Xlint:poly-implicit-overload}
    \item \textit{-Xlint:option-implicit}
    \item \textit{-Xlint:delayedinit-select}
    \item \textit{-Xlint:by-name-right-associative}
    \item \textit{-Xlint:package-object-classes}
    \item \textit{-Xlint:unsound-match}
    \item \textit{-Xlint:stars-align}
  \end{itemize}
  \end{multicols}

  \item \textbf{-Ywarn-adapted-args:} Alias für \textit{-Xlint:adapted-args}. Ist bei \textit{Zeta} nur im \textit{server} Projekt aktiviert.
  \item \textbf{-Ywarn-dead-code:} Warnt vor ungenutzem Programmcode. Ist bei \textit{Zeta} nur im \textit{server} Projekt aktiviert.
  \item \textbf{-Ywarn-inaccessible:} Alias für \textit{-Xlint:inaccessible}. Ist bei \textit{Zeta} nur im \textit{server} Projekt aktiviert.
  \item \textbf{-Ywarn-nullary-override:} Alias für \textit{-Xlint:nullary-override}. Ist bei \textit{Zeta} nur im \textit{server} Projekt aktiviert.
  \item \textbf{-Ywarn-numeric-widen:} Warnt vor der Typenerweiterung eines primitiven Datentyps bei Numerics \cite{scala_numeric_widen}. Ist bei \textit{Zeta} nur im \textit{server} Projekt aktiviert.
\end{itemize}

Aus der Auflistung der genutzten Compiler Flags bei \textit{Zeta} ergibt zunächst erstmal das bei den Projekten \textit{common}, \textit{client} und \textit{shared} kein einziges Compiler Flag gesetzt wird. Außerdem werden einige Flags aufgrund von Aliase mehrfach gesetzt. Dabei ist dies aus Beschreibung der Flags von den Ausgaben des Scala Compilers nicht ersichtlich. Stattdessen ist dies nur im Programmcode des Scala Compilers nachvollziehbar \cite{scala_compiler_warnings}. Dabei scheinen für diese Flags auch zu einen bestimmten Zeitpunkt Deprecated Nachrichten geplant zu sein, da der Aufruf von \textit{withDeprecationMessage} bei der Definition der Aliase auskommentiert ist. Entsprechend diesem Ergebnis sollten die Compiler Flags über alle Projekte vereinheitlicht werden und überflüssige Compiler Flags entfernt werden.

Zusätzlich ist bei \textit{Zeta} noch das Compiler Plugin Wartremover im Einsatz und erweitert den Scala Compiler um zusätzliche Regeln zur Einhaltung von Scala Best Practices oder weist auf bestimmte Konstrukte hin, die zu Problemen mit dem Scala Compiler führen. Der Wartremover hat keine Konfigurationsdatei und wird über die \textit{built.sbt} konfiguriert. Zum Einsatz kommen bis auf die \textit{NonUnitStatements} alle als stabil gelten Regeln oder auch von Wartremover selbst als \textit{unsafe} bezeichneten Regeln. Dabei sind alle vom Wartremover aktivierten Regeln als Warnung konfiguriert. Mit am häufigsten wird die \textit{OptionPartial} Regel in über 20\% aller Warnung im \textit{server} Projekt ausgelöst und soll verhindern das \textit{Option.get} genutzt wird. Das \textit{scala.Option} Konzept bietet die Möglichkeit optionale Werte zu definieren \cite{scala.option}. Dabei ist \textit{scala.Option} eine Alternative zu \textit{null} und ermöglicht per Pattern Matching zu prüfen, ob die Option einen Wert hat oder nicht. Die Nutzung von \textit{Option.get} wiederrum kann mit der Nutzung von \textit{null} ohne eine entsprechende Prüfung gleichgesetzt werden, da eine  Option ohne Wert eine Exception wirft. Entsprechend wird auch die \textit{Null} Regel genutzt um die Nutzung von \textit{null} zu unterbinden. In diesem Fall scheint das Konzept hinter den \textit{scala.Option} entweder bei den bisherigen Implementierung nicht verstanden worden zu sein oder alternativ sind bewusst Technische Schulden aufgebaut worden. Viele der restlichen Warnungen aus dem Wartremover sind über weitere unsaubere Implementierungen wie z.B. die Nutzung von \textit{Any} entstanden und sollten hinreichend abgebaut werden.

Für Scalastyle wird nicht die Standard Konfiguration genutzt, sondern eine eigens erstelle Konfiguration \cite{zeta_new_scalastyle}. Einige der aktivierten Regeln werden als Fehler ausgegeben, dabei wird die Regel \textit{UnderscoreImportChecker} zur Verhinderung von Wildcard Imports z.B. mit über 80\% aller Scalastyle Fehler im \textit{server} Projekt am häufigsten ausgelöst. Wildcard Imports führen zu unvorhergesehenen Nebeneffekten. Indem z.B. durch eine Anpassung eine neue Klasse in einem Paket eingeführt wird und zufällig im Paket der Klasse mit einem Wildcard Import dieses Paketes wird eine Klasse mit denselben Namen aus einem anderen Paket schon benutzt. Dies verursacht einen Fehler beim Kompilieren in der Klasse, in der gar keine Änderung vorgenommen wurde. Des Weiteren wird durch die Wildcard Imports die genaue Anzahl an Abhängigkeiten einer Klasse vor dem Entwickler versteckt und es wird erst viel später sichtbar, wenn eine Klasse zu viele Unterschiedliche Aufgaben vereint, überladen ist und somit auch das \ac{srp} verletzt. Neben der \textit{UnderscoreImportChecker} Regel wird außerdem mit einer Anzahl von etwa 550 rund 20\% aller Scalastyle Warnungen die \textit{PublicMethodsHaveTypeChecker} Regeln im \textit{server} Projekt ausgelöst. In Kombination mit der Information über die Anzahl an Methoden im \textit{server} Projekt aus der Tabelle~\ref{tab:ZETA_METRICS_OLD} auf Seite~\pageref{tab:ZETA_METRICS_OLD} scheinen über 70\% aller Methoden öffentlich zugänglich zu sein. Dazu kommt noch, das die \textit{MethodLengthChecker} Regeln mit einer maximalen Länge von 50 Zeilen pro Methode nur 14 Mal im \textit{server} Projekt ausgelöst wurde. Entsprechend scheint die Implementierung pro Klasse über kleine Methoden modularisiert zu sein. Jedoch scheinen die für den internen Ablauf definierten Methoden zum Großteil auch öffentlich zugänglich zu sein. Damit scheint in weiten Teilen ein Fehlverständnis am Prinzip der Datenkapselung und des Konzepts der Mutator vorzuliegen. Diese Art der Implementierung ist ein Anti-Pattern und wird Object Orgy bezeichnet. Viele der aktivierten Scalastyle Regeln sind ansonsten für die Forcierung eines bestimmten Style Guides zuständig, lassen nur bedingt weitere Interpretationen für Technische Schulden zu. 

Beim Thema Unittests nutzt \textit{Zeta} im \textit{backend} Projekt die Bibliotek \textit{ScalaTest} und im \textit{server} Projekt wird \textit{specs2} verwendet \cite{scalatest_project,specs2}. Zusätzlich zur Analyse wurde im Rahmen dieser Arbeit noch \textit{Scoverage} als \ac{sbt} Plugin hinzugefügt. Zunächst einmal ist die genutzt Version von \textit{ScalaTest} um mindestens ein Minor veraltet, aber auch ein nächstes Major Release steht schon zur Verfügung. Das \textit{backend} Projekt umfasst nur einen einzigen Unittest. Dabei schlägt schon die Kompilierung dieses Tests mit eine Reihe von Fehlern fehl. Die genauen Fehler können im Anhang~\ref{subsec:APPENDIX_LISTINGS_OLD_BACKEND_COMPILE} auf Seite~\pageref{subsec:APPENDIX_LISTINGS_OLD_BACKEND_COMPILE} eingesehen werden. Des Weiteren umfassen die Unittests im \textit{server} Projekt zwei Tests und lassen sich erfolgreich kompilieren. Aber die Tests werfen zur Laufzeit einen Fehler und können aus diesem Grund nicht ausgeführt werden. Der genaue Fehler kann im Anhang~\ref{subsec:APPENDIX_LISTINGS_OLD_SERVER_TEST} auf Seite~\pageref{subsec:APPENDIX_LISTINGS_OLD_SERVER_TEST} eingesehen werden. Schlussendlich lässt sich die Test-Abdeckung aufgrund der genannten Probleme nicht prüfen und wäre aufgrund der geringen Anzahl an Unittests mit großer Wahrscheinlichkeit auch relativ niedrig \cite[Scoverage]{analys_old}.

\subsection{ScalaJS}

Das \textit{client} Projekt von \textit{Zeta} baut, wie zuvor in Abschnitt~\ref{sec:INITIAL_BUILD} ab Seite~\pageref{sec:INITIAL_BUILD} erwähnt, auf \textit{Scala.js} auf. Mit \textit{Scala.js} ist es möglich in Scala geschriebenen Programmcode zu JavaScript zu kompilieren. \textit{ScalaJS} ermöglicht in einer Webapplikation mit dynamische Webseiten den gesamten Programmcode für Back- und Front-End mit Scala zu realisieren. Seine Stärken spielt \textit{Scala.js} aus, wenn die Implementierung primär von in Scala geschriebenen Abhängigkeiten aufbaut. JavaScript Abhängigkeiten können wie zuvor erwähnt über das Facade Konzept in Scala eingebunden werden \cite{scalajs_facade}. Jedoch liegt der Fokus von \textit{Scala.js} z.B. beim Thema Unittests klar auf Scala basierte Testing Frameworks \cite{scalajs_testing}. In Kombination mit einer Implementierung, die primär von JavaScript Bibliotheken abhängt, sind Unittests einschließlich mit Facades nur mit Mocks möglich. Dies wird durch die Rolle der Facades verursacht, da diese nur die Scala Typen der öffentliche \ac{api} einer JavaScript Bibliothek repräsentieren und nicht deren Implementierung enthalten. Damit ermöglichen die Facades die Kompilierung des Scala Programmcodes nach JavaScript. Zur Laufzeit erwartet der von Scalajs generierte JavaScript Code, dass die \acp{api} der Bibliotheken genau in der Form der Facades definiert sind. Unittests mit Facades sind aus diesem Grund nicht möglich und die Tests müssen entsprechend mit dem JavaScript genierten Programmcode durchgeführt werden.

Die Implementierung des \textit{client} Projekt basiert neben \textit{Scala.js} auf den JavaScript Bibliotheken \textit{jQuery} und dem \textit{Ace} Editor. Für \textit{jQuery} wird ein schon existierende Facade benutzt und für den \textit{Ace} Editor wurde ein eigener Facade definiert, obwohl mit \textit{scalajs-ace} schon ein Facade für \textit{Ace} existiert \cite{scalajs_js_libs}. Die im \textit{client} Projekte definierten Facades sind recht umfangreich und sind für einen Großteil der über 500 erstellen Methoden im gesamten Scala Projekt verantwortlich. In der Theorie bieten Facades die Möglichkeit per Scala typsicher auf die JavaScript Bibliotheken zuzugreifen. Je nachdem wie groß und komplex die Public \ac{api} einer Bibliothek ist, können für nicht benutzte Teile der \ac{api} einfach für die Parameter oder Rückgaben der Datentyp js.any oder auch js.dynamic genutzt werden. Je öfter diese benutzt werden desto geringer ist innerhalb von Scala die resultierende Typsicherheit. Außerdem ist bei den angebotenen Facades für bekannte JavaScript Bibliotheken zum Teil nicht ersichtlich für welche Version einer Bibliothek die Typ Informationen gedacht sind, da selten z.B. ein Verweis auf ein WebJar erfolgt. Auch Unittests sind in den wenigsten der angebotenen Facades vorhanden. Bei einem Update einer JavaScript Bibliothek müssten sämtliche Facades auf ihre Kompatibilität überprüft werden. Inkompatibilität der Facades zu den JavaScript Bibliotheken würden nicht während des Build Prozesses, sondern erst zur Laufzeit im Browser aufgrund von Fehlern sichtbar werden.

Zusätzlich baut das \textit{client} Projekt auf die Scala Bibliothek \textit{Scalot} auf. \textit{Scalot} von Jan Moritz Höfner wurde im Rahmen eines früheren Projekts im Jahr 2015 für \textit{Zeta} entwickelt und bietet eine Transformation für Text Operation zum gleichzeitigen Bearbeiten eines Dokuments von mehrere Benutzern \cite{scalot}. Seit der ursprünglichen Erstellung von \textit{Scalot} wurde jedoch keine weiterer Commit oder Release erstellt. Entsprechend ist \textit{Scalot} nicht für Scala 2.12 verfügbar \cite{scalot_releases}. Aus den in \ac{sbt} definierten Abhängigkeiten ist dieser Sonderstatus von \textit{scalot} auch nicht erkennbar gewesen und verhindert nun ein Upgrade auf Scala 2.12. Bei den genutzten Abhängigkeiten für \textit{Zeta} sollte darauf geachtet werden, dass diese Abhängigkeiten von einer größeren Community verwendet werden und mehr als einen Maintainer hat. \textit{Scalot} enthält bis auf Abhängigkeiten für die Unittests keine weiteren Abhängigkeiten und die Implementierung umfasst etwa 600 Zeilen. Für \textit{Scalot} wäre es zu Beginn ausreichend gewesen, ein weiteres \ac{sbt} Unterprojekt innerhalb \textit{Zeta} zu sein. 

Des Weiteren ist das generierte JavaScript Programmcode von \textit{Scala.js} stark aufgebläht und erzeugt bei \textit{Zeta} aus den etwas über 2000 Zeilen des \textit{client}, \textit{shared} und \textit{Scalot} Projekte mit über 40 Tsd. Zeilen eine 20 fach größere JavaScript Datei. Zusätzlich ist die Datei aufgrund der \textit{scalaJSProd} Einstellung nur schwer lesbar. Eine vergleichbare Implementierung direkt per JavaScript könnte etwa mit gleich viel Programmcode wie die Scala Implementierung realisiert werden. Auch die Implementierung innerhalb des \textit{client} Projekts wirkt wenig durchdacht. In der Regel werden per Konstruktor Daten wie z.B. der Identifier eines Models übergeben und nur selten \ac{di} benutzt \cite{zeta_client_controller}. In den wenigen Fällen in den \ac{di} benutzt wird, hängt die Low-Level von der High-Level Implementierung ab wie z.B. die \textit{WebSocketConnection} vom dem \textit{CodeEditorController} und genau umgekehrt. Eine bessere Lösung wäre z.B. funktional über einen Callback oder objektorientiert über den Einsatz des Observer Entwurfsmusters möglich.

Alles in allem wäre aufgrund der aktuellen Lösung zu überlegen, ob eine reine Implementierung per JavaScript die bessere Alternative ist. Dadurch müssten zusätzlich noch die Websocket und Webservice \ac{api} ein unabhängiges Schema bekommen und nicht wie aktuell auf einem internen Kommunikationsprotokoll basieren.

\subsection{SbtWeb Assets}

Das \textit{server} Projekt nutzt, aufgrund des \textit{Play Frameworks}, \textit{SbtWeb} zur Einbindung von JavaScript, \ac{css} und CoffeeScript Dateien. Die Dateien befinden sich bei \textit{Zeta} im Verzeichnis \textit{api/server/app/assets} und enthalten neben eigenen Implementierungen auch vollständige Kopien einiger JavaScript Bibliotheken. Eine Übersicht der JavaScript Assets kann in der Tabelle~\ref{tab:ZETA_WEB_ASSETS_LOC} ab Seite~\pageref{tab:ZETA_WEB_ASSETS_LOC} eingesehen werden. Ein besonderer Fall ist der \textit{Rappid} Editor. Zuerst einmal befindet sich der Programmcode von \textit{Rappid} im Verzeichnis \textit{assets/jointjs} und enthält zusätzlich für \textit{Zeta} geschriebene \textit{JointJS} Erweiterungen im Verzeichnis \textit{assets/jointjs/ext}. Außerdem bietet der Programmcode von \textit{Rappid} keine Hinweise auf die genutzte \textit{Rappid} Version. Anders sieht es jedoch bei \textit{JointJS} Bibliothek aus. Die genutzte Version von \textit{JointJS} lässt sich aufgrund der Kopien auch nicht direkt über \textit{Zeta} bestimmen, aber im Lizenzkopf zu Beginn der Dateien lässt sich die Version von \textit{JointJS} nachvollziehen. \textit{JointJS} wird in der Version 0.9.5 verwendet und hängt damit zum jetzigen Zeitpunkt (Oktober 2017) nun insgesamt zwei Major Versionen hinterher.

Bei der weiteren Analyse ist eine Unstimmigkeit bei der eingesetzten \textit{Backbone} Version aufgefallen. \textit{JointJS} nutzt die Bibliothek \textit{Backbone} in der Version 1.2.1 und der \textit{Rappid} mit seinen Erweiterungen nutzt wiederrum \textit{Backbone} in der Version 1.0.0. Damit ein Parallel-Betrieb der beiden \textit{Backbone} Versionen möglich ist wird zur Laufzeit im Browser die neuere \textit{Backbone} Version global unter dem Namen \textit{backboneNew} referenziert. \textit{JointJS} kann nun auf die neuere \textit{Backbone} Version zugreifen, da zuvor im Programmcode von \textit{JointJS} jede Referenz auf die Variable \textit{backbone} durch \textit{backboneNew} ausgetauscht wurde. Die genauen Gründe für eine derartig tiefgreifende Anpassung lassen sich im Nachhinein nicht mehr ermitteln. Die Vermutung liegt jedoch nahe, dass aufgrund von Problemen mit der vom \textit{Rappid} mitgelieferten \textit{JointJS} Version eine Upgrade auf eine neuere Version unter allen Mitteln notwendig war. Dabei war es wohl nicht möglich \textit{Rappid} im gesamten auf eine neuere Version zu aktualisieren. Anders könnten die massiven Nachteile einer solchen Veränderung des Programmcodes und der einhergehenden Inkompatibilität mit der offiziellen Version von \textit{JointJS} nicht in Kauf genommen werden. Hinzu kommen noch weitere Modifikationen im Rahmen von Fehlerkorrekturen am Programmcode von \textit{JointJS}. Die Fehler beim Autor zu melden und nach der Korrektur auf die neue Version zu aktualisieren wäre die langsamere, aber die korrekte Vorgehensweise gewesen. Übergangsweise wären Fehlerkorrekturen per Patches möglich gewesen. Einhergehend mit zuvor getätigten Anpassungen für \textit{Backbone} würde eine Aktualisierung mit erheblichen zeitlichen Aufwand einhergehen. Mit der Nutzung eines Paket Managers, z.B. per \ac{sbt} mit WebJars oder per \ac{npm}, sollten sich Anpassungen an einer Bibliothek schwieriger gestalten und auf die Nutzung von Kopien des Programmcodes einer Bibliothek sollte verzichtet werden.

\begin{table}[ht]
    \smallskip
    \centering
    \begin{tabular}{| l | r | r | r |}
    \hline
    \bf Projekt & \bf Gesamt & \bf SLOC & \bf CLOC \\ \hline
    Ace Bibliothek & 3164 & 2387 & 369 \\ \hline
    jQuery Bibliothek & 185 & 137 & 22 \\ \hline
    Rappid Bibliothek & 43564 & 25887 & 9248 \\ \hline
    JointJS Erweiterungen & 1703 & 1001 & 473 \\ \hline
    MetaModel Editor & 2205 & 1519 & 332 \\ \hline
    Model Editor & 2245 & 1611 & 252 \\ \hline
    \bf Bibliotheken Gesamt & \bf 46913 & \bf 28411 & \bf 9639 \\ \hline
    \bf Zeta Gesamt & \bf 6153 & \bf 4131 & \bf 1057 \\ \hline
    \end{tabular}
    \caption{Zeta Ausgangszustand - JavaScript \ac{loc} der SbtWeb Assets}
    \label{tab:ZETA_WEB_ASSETS_LOC}
\end{table}

Die restlichen JavaScript Bibliotheken wie z.B. \textit{jQuery} liegen zum Teil nur in minifizieren Varianten vor. Der Prozess des Minifizierens von JavaScript bezeichnet den Vorgang eine JavaScript Programmcode auf eine möglichst komprimierte Variante zu verkleinern und dabei die gleiche Funktionalität beizubehalten. Die Dateigröße des resultierenden JavaScript Dateo kann um ein vielfaches kleiner sein als beim Original. Jedoch ist der Code für Menschen nur noch schwer lesbar und aus diesem Grund ist die Nutzung minifizierter Dateien während der Entwicklung mit mehr Aufwand verbunden. Andererseits ist die minifizierte Variante im einen Produktivbetrieb zu bevorzugen, um den Benutzern der Webapplikation eine möglichst geringe Ladezeit zu bieten. Entsprechend der Umgebung sollte also die minifizierte Variante oder das Original genutzt werden. Das \textit{Play Framework} ist in der Lage zwischen einem für Development und Produktion optimierte Assets zu unterscheiden \cite{play_assets_min}. Dafür müssten aber beide Variante einer Bibliothek vorhanden sein. In der Regel sind beide Varianten im Webjar einer Bibliothek wie z.B. für die eingesetzte \textit{jQuery} Version enthalten \cite{webjar_jquery_files}. 

Des Weiteren wird auf jede JavaScript oder \ac{css} Abhängigkeit in den \ac{html} Dateien wie z.B. bei den graphischen Editoren einzeln verwiesen. Im Extremfall werden aus diesem Grund z.B. im Model-Editor fast 60 unterschiedliche JavaScript Dateien geladen \cite{zeta_model_graphical}. Dies ist im Produktiveinsatz für eine möglichst gute Benutzererfahrung keine optimale Lösung, da eine höhere Anzahl an Anfragen sich negativ auf die Ladezeiten einer Webseite auswirkt. Die ganzen JavaScript Dateien sollten zu einem oder mehrere sogenannte Bundles zusammengefasst werden und im Optimalfall nur eine einzige Anfrage jeweils für JavaScript oder \ac{css} zu benötigen. Ein solches Bundle lässt sich theoretisch über ein Plugin wie z.B. \textit{sbt-concat} realisieren \cite{sbt_concat}. Die Lösung behebt das Problem viele einzelne Dateien im Browser zu laden, aber die genauen Abhängigkeiten zwischen den einzelnen JavaScript Dateien ist nicht ersichtlich. Entsprechend kann jegliche Änderung an der Reihenfolge der Dateien zu einem Fehler im Browser führen.

Ein Schritt in die Richtung einer Lösung dieses Problems bietet das Module Entwurfsmuster zur Modularisierung von JavaScript \cite{javascript_patterns}. Das Module Entwurfsmuster bietet zum einen die Möglichkeit die Implementierung eines Modules außerhalb des globalen Kontext zu definieren und zwischen Public und Private \ac{api} zu unterscheiden. Zusätzlich können die Abhängigkeiten eines Modules von außen per Parameter injected werden. \textit{Zeta} nutzt in der Regel das Module Entwurfsmuster oder das Module System von \textit{Backbone}, aber verzichtet, wie im Programmcode~\ref{lst:JS_MODULE_PATTERN} auf Seite~\pageref{lst:JS_MODULE_PATTERN} zusehen, darauf die Abhängigkeiten explizit anzugeben. Ein weiterentwickelter Ansatz zur Modularisierung von JavaScript bietet \ac{amd} und bietet im Gegensatz zu den Module Patterns die Möglichkeit Abhängigkeiten zwischen den JavaScript Modulen aufzulösen \cite{javascript_patterns}. Eine Implementierung von \ac{amd} bietet z.B. der Module Loader \textit{RequireJS}. Die Implementierungen mit JavaScript für \textit{Zeta} nutzt \ac{amd} nicht und eine Portierung ist aufgrund der nicht angegebenen Abhängigkeiten mit erhöhtem Aufwand verbunden. Das \textit{Play Framework} bietet aber über das \textit{sbt-rjs} Plugin für \textit{SbtWeb} die Möglichkeit \textit{RequireJS} zu nutzen \cite{play_requirejs}. Das \textit{Play Framework} warnt in diesem Zusammenhang gleichzeitig, aber nicht die Standard JavaScript Engine über \ac{jvm} zu benutzen, sondern direkt auf \textit{Node.js} zu setzen. Der externe Einsatz von \textit{Node.js} würde zudem auch die Nutzung weiterer alternativer Module Loader wie z.B. \textit{CommonJS} oder den neuen \ac{es6} Module über einen Module Bundler wie z.B. \textit{Webpack} \cite{webpack_modules} ermöglichen.

\bigskip
\lstinputlisting[caption={[JavaScript Module Pattern - linkTypeSelector.js] JavaScript Module Entwurfsmuster - linkTypeSelector.js \cite{zeta_js_module_linkTypeSelector}},label={lst:JS_MODULE_PATTERN}]{listings/linkTypeSelector.js}
\smallskip

\subsection{Generischer Model-Editor}

\textit{Zeta} nutzt nicht nur eine statisch definierten \textit{Rappid} Instanz wie z.B. beim MetaModel-Editor, sondern aus den Informationen des Meta Modells und der Definitionen über die textuellen \acp{dsl} erzeugt \textit{Zeta} pro Projekt jeweils eine \textit{Rappid} Konfigurationen für die Instanz eines Model-Editors. Die Erzeugung wird durch den \textit{Generate} Knopf in Projektübersicht angestoßen. Auf das Meta Modell kann direkt zugegriffen werden, da dieses vom MetaModel-Editor in einem bestimmten \ac{json} Schema an den \textit{Play Server} übertragen wird und in ein Objekt transformiert werden kann. Bei den Definitionen über die textuellen \acp{dsl} handelt es sich um einen gänzlich anderen Sachverhalt, da diese nicht einfach transformiert werden können. Zu diesem Zweck muss ein Parser zum Einsatz kommen und dieser wird von der Klasse \textit{SprayParser} aus dem \textit{server} Projekt erfüllt \cite{zeta_sprayparser}.

Der \textit{SprayParser} baut auf dem \textit{JavaTokenParser} von Scala auf und gehört zur Klasse der Parser Combinator. Dabei beinhaltet der \textit{SprayParser} die Parser für alle drei \acp{dsl}. Die \acp{dsl} sind hierarchisch aufgebaut, aber die interne Implementierung der Parser ist nicht voneinander abhängig. Aufgrund der geringen Kopplung der verschiedenen Teilbereiche sollte der \textit{SprayParser} in mehrere kleinere Klassen aufgeteilt werden. Außerdem ist bei der öffentlichen \ac{api} der \textit{SprayParser} Klasse die Reihenfolge, in der die Parser aufgerufen werden müssen, nicht ersichtlich. Außerdem ist der \textit{SprayParser} nicht zustandslos und kann deshalb nicht als Service genutzt werden. Die voneinander unabhängigen öffentlichen Methoden und der interne Zustand ist z.B. beim Aufruf der drei Parser in der Klasse \textit{GeneratorController} zusehen \cite{zeta_generatorcontroller}.

\bigskip
\lstinputlisting[caption={[Style Parser - Zeile 33 - 46 aus dem Sprayparser] Style Parser - Zeile 33 - 46 aus dem Sprayparser \cite{zeta_sprayparser}},label={lst:ZETA_STYLE_PARSER}]{listings/style_parser.scala}
\smallskip

Durch den Aufbau der Parser ist es außerdem in vielen Fällen unmöglich kontextspezifische Syntax Fehler auszugeben, da viele Keywords zuerst generisch erkannt werden, aber der zu den Keywords in Beziehung stehende Inhalt erst viel später über eigene Parser geparst wird. Am Beispiel des Parsers für die \textit{Style} \ac{dsl} aus dem Programmcode~\ref{lst:ZETA_STYLE_PARSER} auf Seite~\pageref{lst:ZETA_STYLE_PARSER} ist zu sehen, dass die Methode \textit{styleAttribute} in Zeile 2 nur pro geparstem Attribute eine Zweiertupel mit \textit{v} als String und \textit{a} als String. Dabei wurde für \textit{v} zuvor in der Methode \textit{styleVariable} aus der Zeile 1 ein Regulärer Ausdruck mit den Namen der Eigenschaften eines Styles erzeugt und als Keyword geprüft. Als nächsten wird in Zeile 7 eine List namens \textit{attributes} mit den Tupeln an die Factory Methode der Style Klasse übergeben. Innerhalb der Factory sind nun weitere kleiner Parser für die Definitionen der verschiedenen Style Eigenschaften vorhanden \cite{zeta_style_model}. Mit den kleineren Parsern geht jedoch der Kontext dafür verloren und bei einem Fehler kann z.B. nicht mehr die Zeile und das wievielte Zeichen in einer Zeile angegeben werden. Für das Parsen einer \ac{dsl} müssen Parser mit einem gemeinsamen Kontext vorhanden sein und nicht viele kleinere voneinander getrennte Parser. Der genutzte Ansatz zur Modularisierung der Parser führt dazu, dass die Funktionalität zur Fehlerausgabe nicht wie vom \textit{JavaTokenParser} vorgesehen genutzt werden kann.

Neben der Syntaxprüfung findet im selben Schritt auch die Verknüpfung zwischen den \acp{dsl} und dem Meta Modell statt und bringt dadurch zusätzliche Komplexität in den \textit{SprayParser}. Alternativ findet das Mapping auch zum Teil erst im Modell wie z.B. bei der \textit{DiaShape} Klasse statt. Des Weiteren wird in einem Großteil der Modells auch eine Instanz der \textit{Cache} Klasse mit dem aktuellen Zustand des \textit{SprayParser} übergeben, um die unterschiedlichsten Aktionen auszuführen. Der \textit{SprayParser} müsste zum einen in einen Parser pro \ac{dsl} aufgeteilt werden und dieser Parser sollten dabei jeweils in mehrere Einzelschritte enthalten, wie z.B. parsen, validieren und linken zwischen den Modellen der \acp{dsl}.

Der \textit{SprayParser} ist aber nur ein Hilfsmittel um zur Erzeugung der Konfiguration für den Model-Editor die Definitionen der textuellen \acp{dsl} in einer verarbeitbaren Form zu haben. Als nächsten Schritt folgt in der \textit{GenratorControll} Klasse der Aufruf der verschiedenen Generatoren. Dabei wird als Parameter unter anderem der \textit{Cache} als \textit{hierachyContainer} übergeben oder als Grundlage zur Ermittlung der Daten von anderen Parameter genutzt. Das Ergebnis der Generatoren sind eine Reihe von JavaScript Dateien, die speziell auf \textit{Rappid} zugeschnitten sind. Zur Laufzeit werden die JavaScript Dateien vom Browser für den Model-Editor geladen. Dieser Ansatz ist proprietär und kann durch eine \ac{rest} \ac{api} mit einem verallgemeinerten \ac{json} Schema realisiert werden. Dabei könnte die Erzeugung der Konfiguration für \textit{Rappid} anhand des \ac{json} Schema im Browser stattfinden und gleichzeitig ist dieser Ansatz flexibel genug, um auch mögliche andere Editoren oder Lösungen für andere Plattformen wie z.B. einer Mobile App zu unterstützen.

\subsection{Akka Cluster}

Das \textit{backend} Projekt für die Dienste \textit{b1}, \textit{b2}, \textit{workers} und \textit{developers} basiert, wie zuvor in Abschnitt~\ref{sec:INITIAL_BUILD} ab Seite~\pageref{sec:INITIAL_BUILD} erwähnt, auf einem \textit{Akka Cluster} und die Implementierung von \textit{Zeta} scheint primär auf einem Beispiel Projekt von Lightbend über verteilte Worker aufzubauen \cite{lightbend_distributed_worker}. Desweilen ist der Name des \ac{sbt} Unterprojekts recht allgemein gehalten, da bis auf das \textit{client} Projekte alles in Scala geschriebene sich mit dem Backend beschäftigt und kann auf einem ersten Blick nicht eindeutig den Inhalt des Projekts wiederspiegeln. Ähnlich ist es auch bei dem \textit{server} Projekt des \textit{Play Servers}.

Prinzipiell ist das Entwurfsmuster für die verteilten Worker eine gute Wahl um eine bestimmte Anzahl an Worker vorzuhalten und die, wenn es etwas zu tun gibt, sich ihre Aufgabe beim Master abholen und abarbeiten. Dabei läuft der \textit{Master} Actor jedoch als Singleton und kann laut offizieller Empfehlung recht schnell zu einem Bottleneck werden \cite{akka_singleton}. Dazu kommt noch, dass mit einem Singleton eine permanente Verfügbarkeit nicht garantiert werden kann, da es bei einem Absturz einige Sekunden dauern kann, bis eine neue Instanz auf einer anderen Node gespawnt wird. Für eine bessere Skalierbarkeit sollte entsprechend ermöglicht werden, auf den Singleton zu verzichten und mehrere Instanzen des Master Actor parallel auszuführen.

Zusätzlich kommen auch Persistent Actor beim \textit{Master} und \textit{WorkQueue} Actor zum Einsatz, um den internen Zustand nach einem Absturz wieder herzustellen \cite{akka_persistence}. Dabei ist das Journal zur Persistierung von Events zuständig und die enthaltenen Nachrichten aus dem Journal werden beim Start oder Neustart in den Actor eingespielt, um den vorherigen Zustand wieder herzustellen. Bei \textit{Zeta} ist das Journal mit dem Plugin Shared LevelDB Journal konfiguriert und ermöglicht somit das Journal für die verschiedenen Persistent Actoren im Actor System mehrerer Nodes vorzuhalten. Dabei wird das Shared LevelDB Journal im Master des \textit{b1} und \textit{b2} Dienst und dem \textit{developers} Dienst ausgeführt \cite{zeta_backend_main} und das Journal wird bei diesen Diensten im lokalen Dateisystem gespeichert. Speziell im Zusammenhang mit der Docker Laufzeitumgebung ist die Speicherung im lokalen Dateisystem zu beachten, dass bei der aktuellen Docker Konfiguration nur bei einem Neustart eines Docker Containers die Daten des Journals im Dateisystem des Containers bestehen bleiben. Falls aus irgendeinem Grund der Container gelöscht und ein neuer erstellt wird, gehen die Daten aus dem Dateisystem des Containers verloren. Des Weiteren wird in der Dokumentation vor dem Einsatz des Shared LevelDB Journal als Single Point of Failure gewarnt und sollte nur zu Testzwecken genutzt werden \cite{akka_persistence}. Alternativ stehen aus der Community eine Reihe von Plugins zur Verfügung um z.B. das Journal über \ac{jdbc} asynchron in einer Datenbank zu persistieren \cite{akka_persistence_plugins} \cite{akka_persistence_jdbc}.

Für die Dienste übergreifende Kommunikation zwischen den Actoren im \textit{Akka Cluster} wird bei \textit{Zeta} zum einen der Distributed Pub Sub genutzt. Der Distributed Pub Sub entspricht dabei dem Publish-Subscribe Entwurfsmuster und ermöglicht dynamisch auf Kanäle zu subscriben oder Nachrichten über diese Kanäle zu senden \cite{subcribe_publish_pattern}. Aus diesem Grund läuft die Kommunikation über den \textit{DistributedPubSubMediator} Actor als Message Broker. Außerdem wird für den \textit{Mediator} Actor die Cluster Sharding Funktionalität von Akka verwendet, um  unabhängig vom physischen Standort dem Actor über den logischen Identifier eine Nachrichten zu übermitteln \cite{akka_cluster_sharding}. Somit kann z.B. der \textit{server} Dienst bei Anfragen auf einen Websocket Endpoint diese Anfrage an den Mediator weiterleiten \cite{zeta_server_to_mediator}. Damit eine Verbindung zum \textit{Mediator} Actor im \textit{developers} Dienst möglich ist, tritt der \textit{Play Server} über den \textit{b1} oder \textit{b2} Dienst dem \textit{Akka Cluster} bei \cite{zeta_docker_play_seeds}. Damit wird der \textit{Play Server} eine Node des \textit{Akka Clusters} und wird in Kommunikationsstruktur des \textit{Akka Clusters} integriert. Ein Beitritt in das \textit{Akka Cluster} ist nicht zwingend notwendig, da die Kommunikation nur von \textit{Play Server} zum \textit{Mediator} Actor stattfindet und bis auf Antworten keine Kommunikation zum \textit{Play Server} stattfindet. Ein alternativer Ansatz ist die Nutzung eines Cluster Clients und würde nicht den Overhead eines Cluster Beitritts mit sich bringen \cite{akka_cluster_client}.

Das \ac{di} Entwurfsmuster wird in \textit{Zeta} bei den \textit{Akka Actoren} primär für die Übergabe von Actor Referenzen genutzt und in seltenen Fällen wird auch ein Service übergeben wie z.B. bei den verschiedenen Manager Actoren im \textit{Mediator} Actor \cite{zeta_mediator}. Gleichzeitig wird aber auch eine Instanz für den einen oder anderen Service wie z.B. dem \textit{SyncGatewaySession} im \textit{Mediator} Actor erstellt und ist dabei auch nur eine Abhängigkeit für einen weiteren Service. Neben der stärkeren Nutzung des \ac{di} Entwurfsmusters, würde sich die Nutzung eines \ac{di} Frameworks zum Auflösen der Abhängigkeiten wie z.B. Google Guice im \textit{Play Server} anbieten. Entsprechende Beispiele über den Einsatz von \ac{di} Frameworks bei \textit{Akka Actoren} existieren auch schon seit längerem \cite{akka_actor_di}. Zur Erstellung einer Instanz eines Actors wird bei \textit{Zeta} primär das von Akka empfohlene Factory Method Entwurfsmuster per \textit{props} Methode im Companion Object wie z.B. beim \textit{Mediator} Actor benutzt \cite{scala_companion_object}.

Die Fehlertoleranz ist, wie im Abschnitt~\ref{sec:INITIAL_GENERATOR} ab Seite~\pageref{sec:INITIAL_GENERATOR} beschrieben, ein wichtiger Bestandteil der \textit{Akka Actoren} \cite{akka_fault_tolerance}. Zu diesem Zweck sollten die abhängigen Actoren als Kind Actoren gestartet werden. Dies wird z.B. bei \textit{Zeta} für die verschiedenen Manager im \textit{Mediator} Actor des \textit{developers} Dienst angewendet. Dabei wird aber keine angepasste Supervisor Strategie benutzt. Stattdessen verwendet \textit{Worker} Actor im \textit{workers} Dienst eine angepasste Supervisor Strategie, obwohl dieser, wie in Abbildung~\ref{fig:ZETA_HIERACHY_OLD} auf Seite~\pageref{fig:ZETA_HIERACHY_OLD} zusehen, keine Kind Actoren hat. Dabei verarbeitet z.B. der \textit{DockerWorkExecutor} Actor im selben Dienst, wie zusehen in Abbildung~\ref{fig:ZETA_ACTOR_OLD} auf Seite~\pageref{fig:ZETA_ACTOR_OLD}, nur Nachrichten vom \textit{Worker} Actor und wäre als Kind Actor für den \textit{Worker} Actor prädestiniert. Aktuell findet im \textit{Worker} Actor per \textit{context.watch} schon eine Überwachung von Lifecycle Events wie z.B. die Terminierung des \textit{DockerWorkExecutor} statt \cite{zeta_worker_actor}. Dazu kommt noch das immer die gleiche Menge an \textit{Worker} und \textit{DockerWorkExecutor} Actoren mit dem Start des \textit{workers} Dienst ausgeführt werden \cite{zeta_backend_main}. Entsprechend sollten die Actor Hierarchien und die Supervisor Strategie überdacht werden.

Die Ausführung der Generatoren mit den Transformationsregeln wird bei \textit{Zeta} vom \textit{DockerWorkExecutor} angestoßen und startet in Folge einen Docker Container mit einem der zuvor erzeugten Docker Images. Dabei ist aktuell \textit{Zeta} fest auf die Ausführung der Generator per Docker gebunden. Über das Adapter Pattern z.B. könnte die eigentlich Ausführungsumgebung flexibilisiert werden, um zum einen in einer Entwicklungsumgebung nicht immer das Docker Image nach einer Änderung neu zu erzeugen oder die Generatoren ohne einen weiteren Docker Container in einem Unterprozess innerhalb des \textit{workers} Dienst auszuführen.

Die derzeitige Lösung eines \textit{Akka Clusters} funktioniert, müsste aber für einen längerfristigen Einsatz an den verschiedensten Stellen optimiert werden. Mit einer entsprechenden Testabdeckung würde die gewählte Lösung für die Probleme eines \textit{Akka Clusters} anders aussehen. Dabei ist zu bedenken, dass der \textit{Akka Cluster} mitunter viele verschiedene Funktionalitäten umfasst und eine Einarbeitung mit einem hohen Aufwand verbunden ist. Schnell können hierbei Fehler bei der Implementierung gemacht werden. Erweiterungen und Modifikation am Aufbau des \textit{Akka Clusters} sollten entsprechend immer mit einem tieferen Review abgeschlossen werden. 

\section{Couchbase Datenbank}

Wie zuvor schon erwähnt ist \textit{Zeta} eine Webapplikation und ist somit grundsätzlich nach einer Client-Server Architektur aufgebaut. Somit verfolgt \textit{Zeta} eine Schichtenarchitektur und der Server als Back-End ist aus Sicht des Benutzers eine Blackbox. Bei einer genaueren Betrachtung der verschiedenen Dienste im Back-End enthält \textit{Zeta} mindestens einen dedizierten Datenbank Server und lässt sich somit zu einer Drei-Schichten Architektur erweitern. Dabei repräsentiert der Client als Browser die Präsentationsschicht. Der \textit{Akka Cluster} mit dem \textit{Play Server}, dem \textit{B1}, \textit{developers} und \textit{workers} Dienst ist die Logikschicht. Schlussendlich repräsentieren die MongoDB und der \textit{Couchbase Server} die letzte Schicht, auch als Datenhaltungsschicht bekannt. Hierbei bauen die einzelnen Schichten hierarchisch aufeinander auf. Als unterste Schicht fungiert die Datenhaltungsschicht und auf diese greift die Logikschicht zu. Auf die Logikschicht greift wiederrum die Präsentationsschicht zu. Somit sollte die Datenhaltungsschicht isoliert von der Präsentationsschicht sein. Bei \textit{Zeta} ist die \textit{Couchbase Datenbank}, wie in Abbildung~\ref{fig:ZETA_ARCH_OLD} auf Seite~\pageref{fig:ZETA_ARCH_OLD} zusehen, öffentlich über den zentralen Proxy Dienst erreichbar und wird aktiv im Browser innerhalb der Webapp zur Modifikation des Datenbestands verwendet. 

Zum einem missachtet dieser Ansatz die Schichtenarchitektur durch überspringen der Logikschicht und zum anderen wird auch das Prinzip des Least-Privilege bei dem direkten Zugriff verletzt. \textit{Zeta} ist als Multi-User System konzipiert und somit sind in den Datenbanken nicht nur die Daten vom eigenen Benutzer vorhanden. Sondern auch die Daten aller anderen Benutzer. Der Zugang auf die \textit{Couchbase Datenbank} innerhalb der Webapp hat Lese- und Schreib-Zugriff auf sämtliche Daten von \textit{Zeta}. Eine Einschränkung des Zugriffs auf die \textit{Couchbase Datenbank} erfolgt durch das vorgeschaltete \textit{Couchbase Sync Gateway} mit seiner eingerichteten Benutzerauthentifizierung bei der Public \ac{rest} \ac{api}. Jedoch erteilt der \textit{Play Server} mit jeder erfolgreichen Benutzerauthentifizierung neben der eigentlichen Sitzung per Cookie mit dem Namen \textit{SyncGatewaySession} eine eigene \textit{Sync Gateway} Sitzung mit Vollzugriff. Die Sitzung für diesen Vollzugriff wird per Admin \ac{rest} \ac{api} des \textit{Sync Gateways} erstellt. Die Admin \ac{rest} \ac{api} des \textit{Sync Gateways} läuft auf einem anderen Port als die Public \ac{rest} \ac{api} und besitzt im Gegensatz zur Public \ac{rest} \ac{api} keine Einschränkung über einen Mechanismus zur Authentifizierung. Für die Benutzeranmeldung bei \textit{Zeta} wird die E-Mail Adresse eines aktivierter Benutzer benötigt und ein neuer Benutzer kann mit einer gültigen E-Mail Adresse über das öffentliche Registrierungsformular erstellt werden.

